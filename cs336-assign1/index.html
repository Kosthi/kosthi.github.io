<!doctype html><html lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>[斯坦福CS336] 作业一：构建 Transformer 语言模型 - LoveIt</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel=stylesheet><meta name=Description content="Hugo theme - LoveIt"><meta property="og:url" content="https://koschei.top/cs336-assign1/"><meta property="og:site_name" content="LoveIt"><meta property="og:title" content="[斯坦福CS336] 作业一：构建 Transformer 语言模型"><meta property="og:description" content="为什么系统爱好者都应该学习大模型？ 在当今 AI 技术浪潮中，掌握大模型知识已成为系统开发者的必备技能。通过参与斯坦福 CS336 大模型系统课程，开始从零构建大模型的实践之旅。这门课程很可能在未来 3 年内成为系统领域的标杆课程（正如 CMU 15-445 数据库课程近年来的地位）。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-14T17:43:58+08:00"><meta property="article:modified_time" content="2026-01-30T01:02:00+08:00"><meta property="article:tag" content="CS336"><meta property="article:tag" content="LLM"><meta property="og:image" content="https://koschei.top/logo.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://koschei.top/logo.png"><meta name=twitter:title content="[斯坦福CS336] 作业一：构建 Transformer 语言模型"><meta name=twitter:description content="为什么系统爱好者都应该学习大模型？ 在当今 AI 技术浪潮中，掌握大模型知识已成为系统开发者的必备技能。通过参与斯坦福 CS336 大模型系统课程，开始从零构建大模型的实践之旅。这门课程很可能在未来 3 年内成为系统领域的标杆课程（正如 CMU 15-445 数据库课程近年来的地位）。"><meta name=application-name content="LoveIt"><meta name=apple-mobile-web-app-title content="LoveIt"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://koschei.top/cs336-assign1/><link rel=prev href=https://koschei.top/bpe-optimization/><link rel=next href=https://koschei.top/cs336-assign5/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"[斯坦福CS336] 作业一：构建 Transformer 语言模型","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/koschei.top\/cs336-assign1\/"},"image":[{"@type":"ImageObject","url":"https:\/\/koschei.top\/images\/Apple-Devices-Preview.png","width":3200,"height":2048}],"genre":"posts","keywords":"CS336, LLM","wordcount":10321,"url":"https:\/\/koschei.top\/cs336-assign1\/","datePublished":"2025-06-14T17:43:58+08:00","dateModified":"2026-01-30T01:02:00+08:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"xxxx","logo":{"@type":"ImageObject","url":"https:\/\/koschei.top\/images\/avatar.png","width":960,"height":960}},"author":{"@type":"Person","name":"Koschei"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><canvas id=particles-canvas></canvas><div id=reading-progress-bar></div><style>#particles-canvas{position:fixed;top:0;left:0;width:100%;height:100%;pointer-events:none;z-index:-1;opacity:.6}[theme=dark] #particles-canvas{opacity:.4}#reading-progress-bar{position:fixed;top:0;left:0;width:0%;height:3px;background:linear-gradient(90deg,#6366f1,#8b5cf6,#a855f7,#ec4899);z-index:9999;transition:width 50ms ease-out;box-shadow:0 0 10px rgba(139,92,246,.5)}</style><script>(function(){const e=document.getElementById("particles-canvas"),t=e.getContext("2d");let n=[],i;const l=80,a=["#6366f1","#8b5cf6","#a855f7","#ec4899","#3b82f6","#14b8a6"];function r(){e.width=window.innerWidth,e.height=window.innerHeight}class d{constructor(){this.reset()}reset(){const n=e.width/2,s=e.height/2;this.angle=Math.random()*Math.PI*2;const t=Math.random()*100;this.x=n+Math.cos(this.angle)*t,this.y=s+Math.sin(this.angle)*t,this.speed=.2+Math.random()*.5,this.vx=Math.cos(this.angle)*this.speed,this.vy=Math.sin(this.angle)*this.speed,this.length=8+Math.random()*15,this.width=2+Math.random()*2,this.color=a[Math.floor(Math.random()*a.length)],this.opacity=.3+Math.random()*.5,this.rotation=this.angle,this.pulseSpeed=.01+Math.random()*.02,this.pulseOffset=Math.random()*Math.PI*2}update(){this.x+=this.vx,this.y+=this.vy,this.opacity=.3+Math.sin(Date.now()*this.pulseSpeed+this.pulseOffset)*.2;const t=100;(this.x<-t||this.x>e.width+t||this.y<-t||this.y>e.height+t)&&this.reset()}draw(){t.save(),t.translate(this.x,this.y),t.rotate(this.rotation),t.globalAlpha=this.opacity,t.beginPath(),t.roundRect(-this.length/2,-this.width/2,this.length,this.width,this.width/2),t.fillStyle=this.color,t.fill(),t.restore()}}function c(){n=[];for(let s=0;s<l;s++){const t=new d,o=Math.random()*Math.max(e.width,e.height)/2;t.x=e.width/2+Math.cos(t.angle)*o,t.y=e.height/2+Math.sin(t.angle)*o,n.push(t)}}function s(){t.clearRect(0,0,e.width,e.height);const o=t.createRadialGradient(e.width/2,e.height/2,0,e.width/2,e.height/2,Math.max(e.width,e.height)/2);o.addColorStop(0,"rgba(139, 92, 246, 0.03)"),o.addColorStop(.5,"rgba(99, 102, 241, 0.01)"),o.addColorStop(1,"transparent"),t.fillStyle=o,t.fillRect(0,0,e.width,e.height),n.forEach(e=>{e.update(),e.draw()}),i=requestAnimationFrame(s)}r(),c(),s(),window.addEventListener("resize",()=>{r(),c()}),document.addEventListener("visibilitychange",()=>{document.hidden?cancelAnimationFrame(i):s()});const u=document.getElementById("reading-progress-bar");function o(){const t=window.scrollY,e=document.documentElement.scrollHeight-window.innerHeight,n=e>0?t/e*100:0;u.style.width=n+"%"}window.addEventListener("scroll",o,{passive:!0}),window.addEventListener("resize",o,{passive:!0}),o()})()</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=LoveIt><span class=header-title-pre><i class='far fa-kiss-wink-heart fa-fw' aria-hidden=true></i></span>LoveIt</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>所有文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/categories/documentation/>文档 </a><a class=menu-item href=/about/>关于 </a><a class=menu-item href=https://github.com/dillonzq/LoveIt title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i> </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=搜索文章标题或内容... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题><i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="menu-item language" title=选择语言><i class="fa fa-globe fa-fw" aria-hidden=true></i>
<select class=language-select id=language-select-desktop onchange="location=this.value"><option value=/en/cs336-assign1/>English</option><option value=/cs336-assign1/ selected>简体中文</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=LoveIt><span class=header-title-pre><i class='far fa-kiss-wink-heart fa-fw' aria-hidden=true></i></span>LoveIt</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></div><a class=menu-item href=/posts/ title>所有文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/categories/documentation/ title>文档</a><a class=menu-item href=/about/ title>关于</a><a class=menu-item href=https://github.com/dillonzq/LoveIt title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i></a><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class=menu-item title=选择语言><i class="fa fa-globe fa-fw" aria-hidden=true></i>
<select class=language-select onchange="location=this.value"><option value=/en/cs336-assign1/>English</option><option value=/cs336-assign1/ selected>简体中文</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">[斯坦福CS336] 作业一：构建 Transformer 语言模型</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/kosthi title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Koschei</a></span>&nbsp;<span class=post-category>收录于 <a href=/categories/llm/><i class="far fa-folder fa-fw" aria-hidden=true></i>LLM</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2025-06-14>2025-06-14</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;约 10321 字&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;预计阅读 21 分钟&nbsp;<span id=busuanzi_container_page_pv>
<i class="far fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_page_pv></span>&nbsp;次阅读
</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#为什么系统爱好者都应该学习大模型>为什么系统爱好者都应该学习大模型？</a><ul><li><a href=#作业概览>作业概览</a></li></ul></li><li><a href=#一字节对编码bpe分词器>一、字节对编码（BPE）分词器</a><ul><li><a href=#11-unicode-标准>1.1 Unicode 标准</a></li><li><a href=#12-unicode-编码>1.2 Unicode 编码</a></li><li><a href=#13-bpe-分词器训练实验>1.3 BPE 分词器训练实验</a></li><li><a href=#14-分词器实验>1.4 分词器实验</a></li></ul></li><li><a href=#二transformer-资源核算>二、Transformer 资源核算</a><ul><li><a href=#21-flops-核算基础>2.1 FLOPs 核算基础</a></li><li><a href=#22-gpt-2-xl-资源核算>2.2 GPT-2 XL 资源核算</a><ul><li><ul><li><a href=#1-令牌嵌入层token-embedding>1. 令牌嵌入层（Token Embedding）</a></li><li><a href=#2-单一及总transformer-块的参数共-48-层每层参数相同>2. 单一及总Transformer 块的参数（共 48 层，每层参数相同）</a></li><li><a href=#1多头自注意力mha的参数>（1）多头自注意力（MHA）的参数</a></li><li><a href=#2前馈网络ffnswiglu-架构的参数>（2）前馈网络（FFN，SwiGLU 架构）的参数</a></li><li><a href=#32-个-rmsnorm-层参数>（3）2 个 RMSNorm 层参数</a></li><li><a href=#4transformer-块总参数>（4）Transformer 块总参数</a></li><li><a href=#3-归一化层rmsnorm>3. 归一化层（RMSNorm）</a></li><li><a href=#4-输出投影层lm-head与嵌入层权重不共享>4. 输出投影层（LM Head，与嵌入层权重不共享）</a></li></ul></li></ul></li></ul></li><li><a href=#三训练-transformer-语言模型>三、训练 Transformer 语言模型</a><ul><li><a href=#31-交叉熵损失>3.1 交叉熵损失</a></li><li><a href=#32-学习率调整>3.2 学习率调整</a></li><li><a href=#33-实现-adamw>3.3 实现 AdamW</a></li></ul></li></ul></nav></div></div><div class=content id=content><h2 id=为什么系统爱好者都应该学习大模型>为什么系统爱好者都应该学习大模型？</h2><p>在当今 AI 技术浪潮中，掌握大模型知识已成为系统开发者的必备技能。通过参与<strong>斯坦福 CS336 大模型系统课程</strong>，开始从零构建大模型的实践之旅。这门课程很可能在未来 3 年内成为系统领域的标杆课程（正如 CMU 15-445 数据库课程近年来的地位）。</p><h3 id=作业概览>作业概览</h3><p>本次作业通过以下三个模块实现了一个小型语言模型：</p><ol><li><strong>Tokenizer 设计与实现</strong> — 字节对编码（BPE）分词器</li><li><strong>模型架构编码</strong> — 含 Self-Attention 机制的 Transformer</li><li><strong>优化器开发</strong> — AdamW 优化器</li></ol><blockquote><p>作业地址：<a href=https://github.com/Kosthi/assignment1-basics target=_blank rel="noopener noreffer">Assignment1-Basics GitHub 仓库</a></p></blockquote><p>接下来，我将分享完成作业的一部分细节和心得。</p><hr><h2 id=一字节对编码bpe分词器>一、字节对编码（BPE）分词器</h2><h3 id=11-unicode-标准>1.1 Unicode 标准</h3><p>**Problem（unicode1）：理解 Unicode（1 分）</p><p>(a) <code>chr(0)</code>返回什么 Unicode 字符？</p><p>NULL字符，即ASCII空字符</p><div class="code-block code-line-numbers open" style="counter-reset:code-block 0"><div class="code-header language-bash"><span class=code-title><i class="arrow fas fa-chevron-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title=复制到剪贴板><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>chr</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;</span><span class=se>\x00</span><span class=s1>&#39;</span></span></span></code></pre></div></div><p>(b) 该字符的字符串表示（<code>__repr__()</code>）与其打印表示有何不同？</p><p>repr()函数显示转移序列&rsquo;\\x00&rsquo;，打印什么都不显示，即空字符</p><div class="code-block code-line-numbers open" style="counter-reset:code-block 0"><div class="code-header language-bash"><span class=code-title><i class="arrow fas fa-chevron-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title=复制到剪贴板><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>repr</span><span class=p>(</span><span class=nb>chr</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;&#39;</span><span class=se>\\</span><span class=s2>x00&#39;&#34;</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>print</span><span class=p>(</span><span class=nb>chr</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span></span></span></code></pre></div></div><p>(c) 该字符出现在文本中时会发生什么？可在 Python 解释器中尝试以下代码验证</p><p>空字符虽然在打印时不可见，但仍作为 Python 字符串的一部分，这表明 Python 字符串可以包含不可见的字符，且这些空字符仍然会影响字符串的存储和处理。</p><div class="code-block code-line-numbers open" style="counter-reset:code-block 0"><div class="code-header language-bash"><span class=code-title><i class="arrow fas fa-chevron-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title=复制到剪贴板><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>chr</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;</span><span class=se>\x00</span><span class=s1>&#39;</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>print</span><span class=p>(</span><span class=nb>chr</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=s2>&#34;this is a test&#34;</span> <span class=o>+</span> <span class=nb>chr</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> <span class=o>+</span> <span class=s2>&#34;string&#34;</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;this is a test</span><span class=se>\x00</span><span class=s1>string&#39;</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;this is a test&#34;</span> <span class=o>+</span> <span class=nb>chr</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> <span class=o>+</span> <span class=s2>&#34;string&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>this</span> <span class=ow>is</span> <span class=n>a</span> <span class=n>teststring</span></span></span></code></pre></div></div><h3 id=12-unicode-编码>1.2 Unicode 编码</h3><p><strong>Problem（unicode2）：Unicode 编码（1 分）</strong></p><p>(a) 为什么优先选择在 UTF-8 编码的字节上训练分词器，而非 UTF-16 或 UTF-32？可对比不同编码对各类输入字符串的输出结果。</p><p>训练分词器时处理的是字节序列，UTF-8能更紧凑地表示常见字符，减少序列长度，这对模型训练更高效。而且UTF-8向后兼容ASCII，处理英文文本时特别高效。</p><div class="code-block code-line-numbers open" style="counter-reset:code-block 0"><div class="code-header language-bash"><span class=code-title><i class="arrow fas fa-chevron-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title=复制到剪贴板><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>test_string</span><span class=o>=</span><span class=s2>&#34;你好,世界!&#34;</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>list</span><span class=p>(</span><span class=n>test_string</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=mi>228</span><span class=p>,</span> <span class=mi>189</span><span class=p>,</span> <span class=mi>160</span><span class=p>,</span> <span class=mi>229</span><span class=p>,</span> <span class=mi>165</span><span class=p>,</span> <span class=mi>189</span><span class=p>,</span> <span class=mi>44</span><span class=p>,</span> <span class=mi>228</span><span class=p>,</span> <span class=mi>184</span><span class=p>,</span> <span class=mi>150</span><span class=p>,</span> <span class=mi>231</span><span class=p>,</span> <span class=mi>149</span><span class=p>,</span> <span class=mi>140</span><span class=p>,</span> <span class=mi>33</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>list</span><span class=p>(</span><span class=n>test_string</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;utf-16&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=mi>255</span><span class=p>,</span> <span class=mi>254</span><span class=p>,</span> <span class=mi>96</span><span class=p>,</span> <span class=mi>79</span><span class=p>,</span> <span class=mi>125</span><span class=p>,</span> <span class=mi>89</span><span class=p>,</span> <span class=mi>44</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>22</span><span class=p>,</span> <span class=mi>78</span><span class=p>,</span> <span class=mi>76</span><span class=p>,</span> <span class=mi>117</span><span class=p>,</span> <span class=mi>33</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>list</span><span class=p>(</span><span class=n>test_string</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;utf-32&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=mi>255</span><span class=p>,</span> <span class=mi>254</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>96</span><span class=p>,</span> <span class=mi>79</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>125</span><span class=p>,</span> <span class=mi>89</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>44</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>22</span><span class=p>,</span> <span class=mi>78</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>76</span><span class=p>,</span> <span class=mi>117</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>33</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span></span></span></code></pre></div></div><p>(b) 以下函数意图将 UTF-8 字节串解码为 Unicode 字符串，但存在错误。该函数为何不正确？请提供一个导致错误结果的输入字节串示例。</p><p>该函数不正确，因为它逐字节解码，这对于多字节UTF-8 字符会失败。单独的字节\xe4是无效的 UTF-8字符，会引发UnicodeDecodeError 异常。</p><div class="code-block code-line-numbers" style="counter-reset:code-block 0"><div class="code-header language-bash"><span class=code-title><i class="arrow fas fa-chevron-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title=复制到剪贴板><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>decode_utf8_bytes_to_str_wrong</span><span class=p>(</span><span class=n>bytestring</span><span class=p>:</span> <span class=nb>bytes</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=s2>&#34;&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>([</span><span class=nb>bytes</span><span class=p>([</span><span class=n>b</span><span class=p>])</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>)</span> <span class=k>for</span> <span class=n>b</span> <span class=ow>in</span> <span class=n>bytestring</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>decode_utf8_bytes_to_str_wrong</span><span class=p>(</span><span class=s2>&#34;hello&#34;</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;hello&#39;</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=s2>&#34;你好&#34;</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=sa>b</span><span class=s1>&#39;</span><span class=se>\xe4\xbd\xa0\xe5\xa5\xbd</span><span class=s1>&#39;</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>decode_utf8_bytes_to_str_wrong</span><span class=p>(</span><span class=s2>&#34;你好&#34;</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>Traceback</span> <span class=p>(</span><span class=n>most</span> <span class=n>recent</span> <span class=n>call</span> <span class=n>last</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>File</span> <span class=s2>&#34;&lt;stdin&gt;&#34;</span><span class=p>,</span> <span class=n>line</span> <span class=mi>1</span><span class=p>,</span> <span class=ow>in</span> <span class=o>&lt;</span><span class=n>module</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl>    <span class=kn>import</span> <span class=nn>platform</span>
</span></span><span class=line><span class=cl>  <span class=n>File</span> <span class=s2>&#34;&lt;stdin&gt;&#34;</span><span class=p>,</span> <span class=n>line</span> <span class=mi>2</span><span class=p>,</span> <span class=ow>in</span> <span class=n>decode_utf8_bytes_to_str_wrong</span>
</span></span><span class=line><span class=cl>    <span class=kn>import</span> <span class=nn>sys</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=ne>UnicodeDecodeError</span><span class=p>:</span> <span class=s1>&#39;utf-8&#39;</span> <span class=n>codec</span> <span class=n>can</span><span class=s1>&#39;t decode byte 0xe4 in position 0: unexpected end of data</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>bytes</span><span class=p>([</span><span class=mh>0xe4</span><span class=p>,</span><span class=mh>0xbd</span><span class=p>,</span><span class=mh>0xa0</span><span class=p>])</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;你&#39;</span> <span class=c1>## 汉字“你”占用了3个字节</span></span></span></code></pre></div></div><p>(c) 给出一个无法解码为任何 Unicode 字符的两字节序列。</p><p><code>b'\x80\x81'</code>是无效的，因为在 UTF-8 中，任何以二进制 <code>10</code> 开头的字节（如 <code>0x80</code>）必须是延续字节，但此处它作为首字节出现，前面没有有效的前导字节。</p><div class="code-block code-line-numbers open" style="counter-reset:code-block 0"><div class="code-header language-bash"><span class=code-title><i class="arrow fas fa-chevron-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title=复制到剪贴板><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=sa>b</span><span class=s1>&#39;</span><span class=se>\x80\x81</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>Traceback</span> <span class=p>(</span><span class=n>most</span> <span class=n>recent</span> <span class=n>call</span> <span class=n>last</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>File</span> <span class=s2>&#34;&lt;stdin&gt;&#34;</span><span class=p>,</span> <span class=n>line</span> <span class=mi>1</span><span class=p>,</span> <span class=ow>in</span> <span class=o>&lt;</span><span class=n>module</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl>    <span class=kn>import</span> <span class=nn>platform</span>
</span></span><span class=line><span class=cl><span class=ne>UnicodeDecodeError</span><span class=p>:</span> <span class=s1>&#39;utf-8&#39;</span> <span class=n>codec</span> <span class=n>can</span><span class=s1>&#39;t decode byte 0x80 in position 0: invalid start byte</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=sa>b</span><span class=s1>&#39;</span><span class=se>\xe4\x80\x81</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;䀁&#39;</span> <span class=c1>## 生僻字 䀁(yòu) UTF-8编码 E4 80 81</span></span></span></code></pre></div></div><h3 id=13-bpe-分词器训练实验>1.3 BPE 分词器训练实验</h3><p><strong>预分词前移除特殊标记</strong></p><div class="code-block code-line-numbers open" style="counter-reset:code-block 0"><div class="code-header language-bash"><span class=code-title><i class="arrow fas fa-chevron-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title=复制到剪贴板><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>special_tokens</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;&lt;|endoftext|&gt;&#34;</span><span class=p>,</span> <span class=s2>&#34;&lt;sep&gt;&#34;</span><span class=p>,</span> <span class=s2>&#34;[SPECIAL]&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># 注意先转义再用&#34;|&#34;分割，在正则表达式中&#34;|&#34;表示为或</span>
</span></span><span class=line><span class=cl><span class=o>&lt;|</span><span class=n>endoftext</span><span class=o>|&gt;|&lt;</span><span class=n>sep</span><span class=o>&gt;|</span><span class=p>[</span><span class=n>SPECIAL</span><span class=p>]</span> <span class=c1># &#34;|&#34;.join(special_tokens) 结果</span>
</span></span><span class=line><span class=cl><span class=o>&lt;</span>\<span class=o>|</span><span class=n>endoftext</span>\<span class=o>|&gt;</span>\<span class=o>|&lt;</span><span class=n>sep</span><span class=o>&gt;</span>\<span class=o>|</span>\<span class=p>[</span><span class=n>SPECIAL</span>\<span class=p>]</span> <span class=c1># Wrong: re.escape(&#34;|&#34;.join(special_tokens)) 结果</span>
</span></span><span class=line><span class=cl><span class=o>&lt;</span>\<span class=o>|</span><span class=n>endoftext</span>\<span class=o>|&gt;|&lt;</span><span class=n>sep</span><span class=o>&gt;|</span>\<span class=p>[</span><span class=n>SPECIAL</span>\<span class=p>]</span> <span class=c1># True: &#34;|&#34;.join([re.escape(token) for token in special_tokens])</span></span></span></code></pre></div></div><p><strong>BPE 优化策略</strong></p><p>为了追求高性能，我使用 Pybind11 绑定 C++ 代码：预分词由 Python 处理，BPE 归并过程交给 C++。主要优化包括：</p><ol><li><strong>并行化处理</strong> — 使用 OpenMP 并行统计，避免锁竞争</li><li><strong>惰性删除队列</strong> — 复杂度从 O(NlogN) 降至 O(KlogN)</li><li><strong>增量更新</strong> — 只更新受影响的相邻 pair</li><li><strong>高效数据结构</strong> — 整数 ID 替代字符串，自定义哈希函数</li></ol><p><strong>性能对比</strong>（TinyStoriesV2-GPT4-train 数据集）：</p><table><thead><tr><th>版本</th><th>BPE归并训练</th><th>提升</th></tr></thead><tbody><tr><td>Python</td><td>10min++</td><td>基准</td></tr><tr><td>C++ 未优化</td><td>366s</td><td>~2x</td></tr><tr><td>C++ 优化</td><td>1.08s</td><td><strong>300x</strong></td></tr></tbody></table><blockquote><p>详细的优化原理和实现请阅读：<a href=/posts/bpe-optimization/ rel>BPE 分词器高性能优化：从 10 分钟到 1 秒的实践</a></p></blockquote><p><strong>Problem（train_bpe_tinystories）：在 TinyStories 上训练 BPE（2 分）</strong></p><p>(a) 训练耗时多久、占用多少内存？词汇表中最长的令牌是什么？是否合理？</p><p>28.03s</p><p>10GB</p><p>最长的 token 是 token：" accomplishment"，对应的 id： 7159，长度： 15 个字符（包括前面的空格）</p><p>合理。</p><p>(b) 分析代码性能。分词器训练过程中哪个部分耗时最长？</p><ul><li><strong>N</strong>：去重后的单词总数（distinct words）</li><li><strong>L</strong>：平均单词长度（字符数/初始token数）</li><li><strong>V</strong>：目标词汇表大小</li><li><strong>M</strong>：合并次数 = V - 256 - |special_tokens|（从256个字节token开始）</li><li><strong>K</strong>：特定pair的出现次数</li><li><strong>P</strong>：临时Pair频率统计表</li></ul><p>未优化前耗时最大是BPE归并过程，需要6分钟，时间复杂度为O(M × N × L)，空间复杂度O(N × L + P)。</p><p>优化后只要1s左右，时间复杂度为O(N × L + M)，</p><p>位置索引: O(N × L)，存储每个相邻对的位置，优先级队列：O(P)，总计空间复杂度为O(N × L + P)。优化算法需要额外的位置索引存储，但避免了每轮重新统计的开销。</p><p>优化后耗时最大为预分词过程，16进程并行30s，24进程并行25s，时间复杂度O(N*L/D)，D为进程个数。</p><p><strong>Problem（train_bpe_expts_owt）：在 OpenWebText 上训练 BPE（2 分）</strong></p><p>(a) 在 OpenWebText 数据集上训练字节级 BPE 分词器，词汇表最大大小设为 32,000。将生成的词汇表和合并序列序列化到磁盘以便后续查看。词汇表中最长的令牌是什么？是否合理？</p><p>最长的令牌列表
ID: 25835 | 字节长度: 64 | 内容: &lsquo;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-&rsquo;
ID: 25821 | 字节长度: 64 | 内容: &lsquo;ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ&rsquo;</p><p>(b) 对比在 TinyStories 和 OpenWebText 上训练的分词器。</p><p>有一些 token 本身里面包含换行符 \n ，写文件的时候没有做转义 ，所以「一条合并规则」在文件里被拆成了多行。</p><div class="code-block code-line-numbers open" style="counter-reset:code-block 0"><div class="code-header language-bash"><span class=code-title><i class="arrow fas fa-chevron-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title=复制到剪贴板><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><pre tabindex=0><code>\n The\n           ← 中间这个 \n 是 token 本身，最后那个 \n 是行结束
        （这一行是空的，因为前面那个 \n 把光标移到下一行）
The</code></pre></div><h3 id=14-分词器实验>1.4 分词器实验</h3><p><strong>Problem（tokenizer_experiments）：分词器实验（4 分）</strong></p><p>(a) 从 TinyStories 和 OpenWebText 中各采样 10 个文档。使用之前训练的 TinyStories 分词器（词汇表大小 10K）和 OpenWebText 分词器（词汇表大小 32K），将这些采样文档编码为整数 ID。每个分词器的压缩比（字节数 / 令牌数）是多少？</p><p>TinyStories-10K 分词器在 TinyStories 上的压缩比为 <strong>4.14 字节/令牌</strong>，OpenWebText-32K 分词器在 OpenWebText 上的压缩比为 <strong>4.70 字节/令牌</strong>。</p><p>(b) 用 TinyStories 分词器编码 OpenWebText 采样文档会发生什么？对比压缩比，或定性描述结果。</p><p>使用 TinyStories-10K 分词器编码 OpenWebText 文档时，压缩比降至 <strong>3.26 字节/令牌</strong>，表明更小的词汇表（10K）在面对复杂文本时会产生更多令牌，导致压缩效率降低。</p><p>(c) 估算分词器的吞吐量（如字节 / 秒）。编码 Pile 数据集（825GB 文本）需要多长时间？</p><p>TinyStories-10K 分词器吞吐量约 <strong>626,519.6 字节/秒</strong>，编码 825GB Pile 数据集约需 <strong>16.4 天</strong>；OpenWebText-32K 约 <strong>763,734.4 字节/秒</strong>，约需 <strong>13.4天</strong>。</p><p>(d) 使用 TinyStories 和 OpenWebText 分词器，分别将对应的训练集和开发集编码为整数令牌 ID（后续用于训练语言模型）。建议将令牌 ID 序列化为<code>uint16</code>类型的 NumPy 数组。为什么<code>uint16</code>是合适的选择？</p><p>两个分词器的词汇表大小（10K 和 32K）均小于 65,536（2¹⁶），因此 uint16 足以表示所有令牌 ID，且比 uint32 节省 50% 存储空间。</p><hr><h2 id=二transformer-资源核算>二、Transformer 资源核算</h2><h3 id=21-flops-核算基础>2.1 FLOPs 核算基础</h3><p>了解 Transformer 各组成部分的计算量和内存占用情况十分有用。我们将逐步开展基础的“浮点运算次数（FLOPs）核算”。</p><p>Transformer 的绝大多数浮点运算都来自矩阵乘法，因此核心思路很简单：</p><ol><li>列出 Transformer 前向传播过程中所有的矩阵乘法操作</li><li>将每个矩阵乘法转换为所需的浮点运算次数</li></ol><blockquote><p><strong>矩阵乘法 FLOPs 规则</strong>：给定矩阵 $A \in \mathbb{R}^{m \times n}$ 和 $B \in \mathbb{R}^{n \times p}$，矩阵乘法 $AB$ 需消耗 $2mnp$ 个 FLOPs。因为 $(AB)[i,j] = A[i,:] \cdot B[:,j]$ 包含 $n$ 次加法和 $n$ 次乘法，共 $2n$ 个浮点运算；而矩阵 $AB$ 共有 $m \times p$ 个元素，因此总 FLOPs 为 $2mnp$。</p></blockquote><h3 id=22-gpt-2-xl-资源核算>2.2 GPT-2 XL 资源核算</h3><p><strong>Problem（transformer_accounting）：Transformer 语言模型资源核算（5 分）</strong></p><p><strong>(a) GPT-2 XL 可训练参数计算</strong></p><p>考虑 GPT-2 XL 模型配置：</p><ul><li>词汇表大小（vocab_size）：50,257</li><li>上下文长度（context_length）：1,024</li><li>层数（num_layers）：48</li><li>模型维度（d_model）：1,600</li><li>注意力头数（num_heads）：25</li><li>前馈网络内层维度（d_ff）：6,400</li></ul><p>若按此配置构建模型，该模型将包含多少个可训练参数？假设每个参数采用单精度浮点数（single-precision floating point）表示，仅加载该模型需要占用多少内存？</p><h5 id=1-令牌嵌入层token-embedding>1. 令牌嵌入层（Token Embedding）</h5><p>作用：将整数令牌 ID 映射为 d_model 维向量，参数是嵌入矩阵。</p><ul><li>矩阵维度：<code>vocab_size × d_model</code>（词汇表中每个令牌对应一个 d_model 维向量）</li><li>计算：50257 × 1600 = 80,411,200（约 8041 万）</li></ul><h5 id=2-单一及总transformer-块的参数共-48-层每层参数相同>2. 单一及总Transformer 块的参数（共 48 层，每层参数相同）</h5><h5 id=1多头自注意力mha的参数>（1）多头自注意力（MHA）的参数</h5><p>MHA 包含 Q/K/V 投影、输出投影 4 个权重矩阵，且<code>d_k = d_model / num_heads = 1600 / 25 = 64</code>（每个头的维度）：</p><ul><li>Q/K/V 投影矩阵（3 个）：每个矩阵维度<code>d_model × d_model</code>（因需将 d_model 维向量拆分为 num_heads 个 d_k 维向量，等价于<code>d_model × (num_heads×d_k) = d_model×d_model</code>）<ul><li>单个投影矩阵参数：1600 × 1600 = 2,560,000</li><li>3 个总参数：3 × 2,560,000 = 7,680,000</li></ul></li><li>输出投影矩阵（1 个）：维度<code>d_model × d_model</code>（将 num_heads 个 d_k 维向量拼接后的 d_model 维向量，映射回 d_model 维）<ul><li>参数：1600 × 1600 = 2,560,000</li></ul></li><li>MHA 单一层总参数：7,680,000 + 2,560,000 = 10,240,000（1024 万）</li></ul><h5 id=2前馈网络ffnswiglu-架构的参数>（2）前馈网络（FFN，SwiGLU 架构）的参数</h5><p>FFN 包含 3 个权重矩阵（W1、W2、W3），维度分别为<code>d_ff×d_model</code>、<code>d_model×d_ff</code>和<code>d_ff×d_model</code>：</p><ul><li><p>W1（输入→内层）：6400 × 1600 = 10,240,000（1024 万）</p></li><li><p>W2（内层→输出）：1600 × 6400 = 10,240,000（1024 万）</p></li><li><p>W3（输入→内层）：6400 × 1600 = 10,240,000（1024 万）</p></li><li><p>FFN 单一层总参数：10,240,000 * 3 = 30,720,000（3072 万）</p></li></ul><h5 id=32-个-rmsnorm-层参数>（3）2 个 RMSNorm 层参数</h5><ul><li>每个 RMSNorm 的增益参数 $g$ 维度：(d_model, )，(1600 个参数 / 层）</li><li>2 个 RMSNorm 总参数：$2 \times 1600 = 3,200$（3200 个 / 层）</li><li>预归一化 Transformer 块包含 <strong>2 个 RMSNorm 层</strong>：分别在 MHA 前和 SwiGLU 前。</li></ul><h5 id=4transformer-块总参数>（4）Transformer 块总参数</h5><p>ROPE 没有需要训练的参数，不变参数全部预计算并缓存。</p><p>计算：MHA 参数 + FFN 参数 + RMSNorm 参数 = 10,240,000 + 30,720,000 + 3,200 = 40,963,200（4096.3200 万 / 层）
48 层总参数：48 × 40,963,200 = 1,966,233,600（约 19.66 亿）</p><h5 id=3-归一化层rmsnorm>3. 归一化层（RMSNorm）</h5><ul><li>RMSNorm 的增益参数 $g$ 维度：(d_model, )，(1600 个参数 / 层）</li></ul><h5 id=4-输出投影层lm-head与嵌入层权重不共享>4. 输出投影层（LM Head，与嵌入层权重不共享）</h5><p>作用：将 Transformer 输出的 d_model 维向量映射到词汇表维度，预测下一个令牌。</p><ul><li><p>矩阵维度：<code>vocab_size × d_model</code></p></li><li><p>计算：50257 × 1600 = 80,411,200（约 8041 万）</p></li></ul><p>将所有组件参数相加：</p><p>$$
\begin{align*}
\text{总参数} &= 80,411,200 + 1,966,233,600 + 1,600 + 80,411,200 \
&= 2,127,057,600
\end{align*}
$$</p><blockquote><p><strong>答案</strong>：总可训练参数约 <strong>21.28 亿</strong>，加载该模型需要占用约 <strong>8 GB</strong> 内存。</p></blockquote><p>(b) 明确完成 GPT-2 XL 型模型前向传播所需的所有矩阵乘法操作。这些矩阵乘法总共需要多少浮点运算次数（FLOPs）？假设输入序列长度等于上下文长度（context_length）。列出所有矩阵乘法操作（含描述），并给出总浮点运算次数。</p><table><thead><tr><th>矩阵乘法操作</th><th>维度说明</th></tr></thead><tbody><tr><td>词嵌入投影（Embedding）</td><td>通常只是查表，无矩阵乘法</td></tr><tr><td>多头注意力 Q/K/V 投影</td><td>每层：(seq_len, d_model) × (d_model, d_model) -> (seq_len, d_model)（Q、K、V 各 1 次，共 3 次）</td></tr><tr><td>点积注意力分数计算</td><td>每层：(seq_len, d_k) × (d_k, seq_len) -> (seq_len, seq_len)</td></tr><tr><td>点积注意力值加权求和计算</td><td>每层：(seq_q, seq_k) × (seq_k, d_v) -> (seq_q, d_v)</td></tr><tr><td>多头注意力输出投影</td><td>每层：(seq_len, d_model) × (d_model, d_model) -> (seq_len, d_model)</td></tr><tr><td>前馈网络第一层（W1）</td><td>每层：(seq_len, d_model) × (d_model, d_ff) -> (seq_len, d_ff)</td></tr><tr><td>前馈网络第二层（W2）</td><td>每层：(seq_len, d_ff) × (d_ff, d_model) -> (seq_len, d_model)</td></tr><tr><td>前馈网络第二层（W3）</td><td>每层：(seq_len, d_model) × (d_model, d_ff) -> (seq_len, d_ff)</td></tr><tr><td>输出层投影（LM Head）</td><td>(seq_len, d_model) × (d_model, vocab_size) -> (seq_len, vocab_size)</td></tr></tbody></table><p>当 seq_len = context_length = 1024，GPT-2 XL一次完整的前向传播（序列长度1024）大约需要 <strong>4.20万亿次浮点运算（4.20 TFLOPs)</strong>。</p><p>(c) 根据上述分析，模型的哪些部分消耗的浮点运算次数（FLOPs）最多？</p><p>FLOPs分布:</p><ul><li>注意力Q/K/V投影: 17.96%</li><li>点积注意力分数计算: 0.15%</li><li>点积注意力权重值计算: 0.15%</li><li>注意力输出投影: 5.99%</li><li>前馈网络第一层: 23.94%</li><li>前馈网络第二层: 23.94%</li><li>前馈网络第三层: 23.94%</li><li>输出层: 3.92%</li><li>前馈网络总计: 71.83%</li><li>注意力总计: 24.25%</li></ul><p>从以上计算可以看出，<strong>前馈网络</strong>是计算量最大的部分，在 GPT-2 XL 中约占总 FLOPs 的71.83%。这主要因为其内部有一个从 <code>d</code> 到 <code>d_ff</code>（通常4倍于<code>d</code>）的大维度矩阵乘法。其次是**注意力模块中的Q/K/V投影，**约占总 FLOPs 的17.96%。</p><p>(d) 对 GPT-2 小模型（12 层、d_model=768、12 个注意力头）、GPT-2 中模型（24 层、d_model=1024、16 个注意力头）和 GPT-2 大模型（36 层、d_model=1280、20 个注意力头）重复上述分析。随着模型规模增大，Transformer 语言模型的哪些部分占总浮点运算次数（FLOPs）的比例会增加或减少？针对每个模型，给出各组件及其对应的浮点运算次数占前向传播总浮点运算次数的比例；此外，用 1-2 句话描述模型规模变化如何影响各组件浮点运算次数的占比。</p><p>下表计算了不同规模的GPT-2模型在序列长度为 1024 时，各组件 FLOPs 占总量的比例。</p><table><thead><tr><th style=text-align:left>模型组件</th><th style=text-align:left>GPT-2 Small</th><th style=text-align:left>GPT-2 Medium</th><th style=text-align:left>GPT-2 Large</th><th style=text-align:left>GPT-2 XL</th><th style=text-align:left>趋势说明</th></tr></thead><tbody><tr><td style=text-align:left><strong>多头注意力 Q/K/V投影</strong></td><td style=text-align:left>13.84%</td><td style=text-align:left>16.51%</td><td style=text-align:left>17.47%</td><td style=text-align:left>17.96%</td><td style=text-align:left><strong>占比小幅增加并趋于稳定</strong>。其计算量（<code>~6SLd²</code>）与模型维度<code>d²</code>成正比，增长速度快于与<code>d</code>成正比的组件，但慢于前馈网络。</td></tr><tr><td style=text-align:left><strong>点积注意力分数计算</strong></td><td style=text-align:left>0.51%</td><td style=text-align:left>0.34%</td><td style=text-align:left>0.23%</td><td style=text-align:left>0.15%</td><td style=text-align:left><strong>占比急剧下降</strong>。其计算量（<code>~2S²Ld_k</code>）仅与序列长度<code>S²</code>和头维度<code>d_k</code>相关，当<code>d</code>增大而<code>S</code>固定时，占比被显著稀释。</td></tr><tr><td style=text-align:left><strong>点积注意力加权求和计算</strong></td><td style=text-align:left>0.51%</td><td style=text-align:left>0.34%</td><td style=text-align:left>0.23%</td><td style=text-align:left>0.15%</td><td style=text-align:left>趋势同“分数计算”，原因完全相同。</td></tr><tr><td style=text-align:left><strong>多头注意力输出投影</strong></td><td style=text-align:left>4.61%</td><td style=text-align:left>5.50%</td><td style=text-align:left>5.82%</td><td style=text-align:left>5.99%</td><td style=text-align:left><strong>占比小幅增加并趋于稳定</strong>。原因同Q/K/V投影，计算量（<code>~2SLd²</code>）与<code>d²</code>成正比。</td></tr><tr><td style=text-align:left><strong>前馈网络 (三层)</strong></td><td style=text-align:left>55.36%</td><td style=text-align:left>66.04%</td><td style=text-align:left>69.89%</td><td style=text-align:left>71.83%</td><td style=text-align:left><strong>占比显著且持续增加</strong>，是总FLOPs的绝对主体。其计算量（<code>~2SLd·d_ff</code>，通常<code>d_ff=4d</code>）与<code>d²</code>成正比，且因层数<code>L</code>的累加效应，增长最快。</td></tr><tr><td style=text-align:left><strong>输出层投影</strong></td><td style=text-align:left>25.16%</td><td style=text-align:left>11.25%</td><td style=text-align:left>6.35%</td><td style=text-align:left>3.92%</td><td style=text-align:left><strong>占比断崖式下降</strong>。该操作仅一次，计算量（<code>~2SdV</code>）仅与<code>d</code>线性相关，远慢于随<code>L</code>线性增长的层内计算，故相对重要性迅速降低。</td></tr></tbody></table><p>基于上表数据，可以清晰地观察到以下趋势：</p><ul><li><strong>前馈网络的FLOPs占比显著增加</strong>：从Small模型的 <strong>55.36%</strong> 增长到XL模型的 <strong>71.83%</strong>。这是因为前馈网络的计算成本（<code>~2 * S * L * d_model * d_ff</code>）与模型维度 <code>d_model</code> 和其内部维度 <code>d_ff</code>（通常为4倍）的乘积强相关，而 <code>d_model</code> 和层数 <code>L</code> 的同步增长会使其计算量增速超过其他部分。</li><li><strong>注意力分数及加权求和计算的FLOPs占比急剧下降</strong>：从Small模型的 <strong>1.02%</strong> 下降到XL模型的 <strong>0.30%</strong>。这是因为该部分计算成本与层数 <code>L</code>、序列长度平方（<code>S²</code>）和注意力头维度（<code>d_k</code>）相关（<code>~2 * S² * L * d_k</code>），而与模型主体维度 <code>d_model</code> 无关。由<code>d_k</code>=<code>d_model</code>/<code>num_heads</code>，可发现 3 个模型的<code>d_k</code>均为 <strong>64</strong>，故该部分计算成本实际仅与<code>L</code>线性相关。考虑一次前向传播，当 <code>d_model</code> 增大而 <code>S</code> 和<code>d_k</code>固定时，该值不变，其占比自然被稀释。再考虑多层，占比不变，而由于加入输出层投影导致分母增大，占比在总体中更小。</li><li><strong>输出层投影的FLOPs占比大幅下降</strong>：从Small模型的 <strong>25.16%</strong> 骤降至XL模型的 <strong>3.92%</strong>。这是因为输出层（<code>(S, d) × (d, V)</code>）仅在整个模型末端计算一次，其FLOPs（<code>~2 * S * d_model * V</code>）仅随 <code>d_model</code> 线性增长。而层内的计算（注意力、前馈）会随层数 <code>L</code> 线性增长，导致输出层的相对贡献迅速变小。</li></ul><p><strong>核心结论</strong>：随着Transformer模型规模的扩大（增加层数和维度），计算瓶颈会从<strong>与模型维度平方相关的前馈网络</strong>和<strong>单次的输出投影</strong>，快速转移到<strong>前馈网络</strong>上。这使得大模型在长序列推理时，前馈网络成为绝对的FLOPs消耗主体。</p><ol><li><strong>主导部分转移</strong>：随着<strong>与词汇表大小<code>V</code>强相关的输出层占比越来越小</strong>，计算主力彻底转变为<strong>与模型维度平方<code>d²</code>强相关的前馈网络</strong>。</li><li><strong>注意力“轻量化”</strong>：<strong>注意力核心计算（分数与加权）</strong> 的占比变得微乎其微，尤其在长模型、短上下文场景下。</li><li><strong>扩展启示</strong>：该趋势解释了为何优化大模型推理时，<strong>降低前馈层的计算/存储开销</strong>（如使用MoE、量化）是关键方向；而在处理极长序列时，<strong>注意力计算的<code>O(S²)</code>复杂度</strong>会重新成为瓶颈。</li></ol><p>(e) 针对 GPT-2 XL 模型，将上下文长度（context_length）增加到 16,384。此时，一次前向传播的总浮点运算次数（FLOPs）会发生怎样的变化？各组件浮点运算次数的相对贡献会如何变化？</p><p>当上下文长度从 1,024 增加到 16,384 时：</p><ul><li>总 FLOPs 从约 3.5T 增加到约 70.4T，增长了约 20 倍（不是简单的线性增长，因为注意力计算包含 S² 项）</li><li>各组件相对贡献变化：<ul><li>自注意力模块的 FLOPs 占比从约 24.25% 增加到约 27.58%（包含注意力分数计算部分）</li><li>前馈网络（FFN）的占比从约 71.83% 下降到约 68.68%</li><li>语言模型头的占比从约 3.92% 下降到约 3.75%</li></ul></li></ul><hr><h2 id=三训练-transformer-语言模型>三、训练 Transformer 语言模型</h2><h3 id=31-交叉熵损失>3.1 交叉熵损失</h3><p>Transformer 语言模型会为长度为 $m+1$ 的序列 $x$ 和每个位置 $i=1,\ldots,m$ 定义分布 $p_{\theta}(x_{i+1} | x_{1:i})$。</p><p>给定由长度为 $m$ 的序列组成的训练集 $D$，我们定义标准的交叉熵（负对数似然）损失函数：</p><p>$$
\ell(\theta; D) = \frac{1}{|D|} \sum_{x \in D} \sum_{i=1}^{m} -\log p_{\theta}(x_{i+1} | x_{1:i})
$$</p><p>Transformer 的单次前向传播会同时输出所有 $i=1,\ldots,m$ 对应的 $p_{\theta}(x_{i+1} | x_{1:i})$，其中 $|D|$ 为训练集大小 (batch_size)。</p><p>具体来说，Transformer 会为每个位置 $i$ 计算对数几率（logits）$o_i \in \mathbb{R}^{V}$，由此可得：</p><p>$$
p(x_{i+1} | x_{1:i}) = \text{softmax}(o_i)[x_{i+1}] = \frac{\exp(o_i[x_{i+1}])}{\sum_{a=1}^{V} \exp(o_i[a])}
$$</p><p>交叉熵损失通常基于对数几率向量 $o_i \in \mathbb{R}^{V}$ 和目标值 $x_{i+1}$ 定义。与 softmax 类似，实现交叉熵损失时需要注意数值稳定性问题。</p><p>我们已经实现了 softmax 的稳定版本，对其取自然对数得 log_softmax：</p><p>$$
\log\left(\frac{e^{x_i}}{\sum_j e^{x_j}}\right) = \log\left(\frac{e^{x_i-m}}{\sum_j e^{x_j-m}}\right) = (x_i - m) - \log\sum_j e^{x_j-m}
$$</p><p><strong>Problem（cross_entropy）：实现交叉熵</strong></p><div class="code-block code-line-numbers" style="counter-reset:code-block 0"><div class="code-header language-bash"><span class=code-title><i class="arrow fas fa-chevron-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title=复制到剪贴板><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>.softmax</span> <span class=kn>import</span> <span class=n>log_softmax</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>cross_entropy</span><span class=p>(</span><span class=n>inputs</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>targets</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># inputs: (batch_size, vocab_size)</span>
</span></span><span class=line><span class=cl>    <span class=c1># targets: (batch_size, )</span>
</span></span><span class=line><span class=cl>    <span class=c1># (batch_size, vocab_size)</span>
</span></span><span class=line><span class=cl>    <span class=n>D</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 1. 计算softmax概率</span>
</span></span><span class=line><span class=cl>    <span class=n>probs</span> <span class=o>=</span> <span class=n>log_softmax</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 2. 提取目标位置的概率</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=n>probs</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>D</span><span class=p>),</span> <span class=n>targets</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 3. 计算平均损失（除以batch_size）</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=o>-</span><span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>p</span><span class=p>)</span></span></span></code></pre></div></div><h3 id=32-学习率调整>3.2 学习率调整</h3><p><strong>Problem（learning_rate_tuning）：调整学习率</strong></p><p>如前所述，学习率是影响训练效果最重要的超参数之一。在我们的简单示例中实际验证这一点：使用上述 SGD 示例，分别尝试另外三个学习率值（1e1、1e2、1e3），仅训练 10 次迭代。观察每个学习率对应的损失变化：是衰减更快、更慢，还是发散（即训练过程中损失增加）？</p><div class="code-block code-line-numbers" style="counter-reset:code-block 0"><div class="code-header language-bash"><span class=code-title><i class="arrow fas fa-chevron-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title=复制到剪贴板><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>(</span>cs336-basics<span class=o>)</span> <span class=o>(</span>base<span class=o>)</span> koschei@192 assignment1-basics % uv run ./cs336_basics/sgd.py
</span></span><span class=line><span class=cl>learing rate: 10.0
</span></span><span class=line><span class=cl>step1 , loss: 20.765413284301758
</span></span><span class=line><span class=cl>step2 , loss: 13.289864540100098
</span></span><span class=line><span class=cl>step3 , loss: 9.796720504760742
</span></span><span class=line><span class=cl>step4 , loss: 7.66488790512085
</span></span><span class=line><span class=cl>step5 , loss: 6.208559036254883
</span></span><span class=line><span class=cl>step6 , loss: 5.14760684967041
</span></span><span class=line><span class=cl>step7 , loss: 4.341323375701904
</span></span><span class=line><span class=cl>step8 , loss: 3.709784507751465
</span></span><span class=line><span class=cl>step9 , loss: 3.203690767288208
</span></span><span class=line><span class=cl>step10, loss: 2.7907707691192627
</span></span><span class=line><span class=cl><span class=o>(</span>cs336-basics<span class=o>)</span> <span class=o>(</span>base<span class=o>)</span> koschei@192 assignment1-basics % uv run ./cs336_basics/sgd.py
</span></span><span class=line><span class=cl>learing rate: 100.0
</span></span><span class=line><span class=cl>step1 , loss: 29.470537185668945
</span></span><span class=line><span class=cl>step2 , loss: 29.470535278320312
</span></span><span class=line><span class=cl>step3 , loss: 5.0563435554504395
</span></span><span class=line><span class=cl>step4 , loss: 0.12100967764854431
</span></span><span class=line><span class=cl>step5 , loss: 1.2657409207190063e-16
</span></span><span class=line><span class=cl>step6 , loss: 1.4107469187345678e-18
</span></span><span class=line><span class=cl>step7 , loss: 4.750480809550992e-20
</span></span><span class=line><span class=cl>step8 , loss: 2.8298939276754454e-21
</span></span><span class=line><span class=cl>step9 , loss: 2.427666298147088e-22
</span></span><span class=line><span class=cl>step10, loss: 2.697406997941209e-23
</span></span><span class=line><span class=cl><span class=o>(</span>cs336-basics<span class=o>)</span> <span class=o>(</span>base<span class=o>)</span> koschei@192 assignment1-basics % uv run ./cs336_basics/sgd.py
</span></span><span class=line><span class=cl>learing rate: 1000.0
</span></span><span class=line><span class=cl>step1 , loss: 23.761384963989258
</span></span><span class=line><span class=cl>step2 , loss: 8577.859375
</span></span><span class=line><span class=cl>step3 , loss: 1481531.25
</span></span><span class=line><span class=cl>step4 , loss: 164804544.0
</span></span><span class=line><span class=cl>step5 , loss: 13349167104.0
</span></span><span class=line><span class=cl>step6 , loss: 842485268480.0
</span></span><span class=line><span class=cl>step7 , loss: 43250442305536.0
</span></span><span class=line><span class=cl>step8 , loss: 1860819142836224.0
</span></span><span class=line><span class=cl>step9 , loss: 6.858582164871578e+16
</span></span><span class=line><span class=cl>step10, loss: 2.2023668704120668e+18</span></span></code></pre></div></div><blockquote><p><strong>答案</strong>：学习率 10 时损失缓慢衰减；学习率 100 时损失迅速衰减，极速收敛至接近零；学习率 1000 时损失爆炸式增长，明显发散。</p></blockquote><h3 id=33-实现-adamw>3.3 实现 AdamW</h3><p><strong>Problem (adamw)：实现 AdamW</strong></p><div class="code-block code-line-numbers" style="counter-reset:code-block 0"><div class="code-header language-bash"><span class=code-title><i class="arrow fas fa-chevron-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title=复制到剪贴板><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>AdamW</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Optimizer</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>params</span><span class=p>,</span> <span class=n>lr</span><span class=o>=</span><span class=mf>1e-3</span><span class=p>,</span> <span class=n>betas</span><span class=o>=</span><span class=p>(</span><span class=mf>0.9</span><span class=p>,</span> <span class=mf>0.999</span><span class=p>),</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-8</span><span class=p>,</span> <span class=n>weight_decay</span><span class=o>=</span><span class=mf>1e-2</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>lr</span> <span class=o>&lt;</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;无效的学习率：</span><span class=si>{</span><span class=n>lr</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>defaults</span> <span class=o>=</span> <span class=nb>dict</span><span class=p>(</span><span class=n>lr</span><span class=o>=</span><span class=n>lr</span><span class=p>,</span> <span class=n>betas</span><span class=o>=</span><span class=n>betas</span><span class=p>,</span> <span class=n>eps</span><span class=o>=</span><span class=n>eps</span><span class=p>,</span> <span class=n>weight_decay</span><span class=o>=</span><span class=n>weight_decay</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>defaults</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>closure</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>closure</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=n>closure</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>group</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>param_groups</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 获取学习率等参数</span>
</span></span><span class=line><span class=cl>            <span class=n>alpha</span> <span class=o>=</span> <span class=n>group</span><span class=p>[</span><span class=s2>&#34;lr&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>beta1</span><span class=p>,</span> <span class=n>beat2</span> <span class=o>=</span> <span class=n>group</span><span class=p>[</span><span class=s2>&#34;betas&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>eps</span> <span class=o>=</span> <span class=n>group</span><span class=p>[</span><span class=s2>&#34;eps&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>weight_decay</span> <span class=o>=</span> <span class=n>group</span><span class=p>[</span><span class=s2>&#34;weight_decay&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>group</span><span class=p>[</span><span class=s2>&#34;params&#34;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>p</span><span class=o>.</span><span class=n>grad</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=k>continue</span>
</span></span><span class=line><span class=cl>                <span class=c1># 获取与参数 p 相关的状态</span>
</span></span><span class=line><span class=cl>                <span class=n>state</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>state</span><span class=p>[</span><span class=n>p</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># 从状态中获取迭代次数，若无则初始化为 1</span>
</span></span><span class=line><span class=cl>                <span class=n>t</span> <span class=o>=</span> <span class=n>state</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;t&#34;</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=c1># 一阶矩估计</span>
</span></span><span class=line><span class=cl>                <span class=n>m</span> <span class=o>=</span> <span class=n>state</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;m&#34;</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>p</span><span class=p>))</span>
</span></span><span class=line><span class=cl>                <span class=c1># 二阶矩估计</span>
</span></span><span class=line><span class=cl>                <span class=n>v</span> <span class=o>=</span> <span class=n>state</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;v&#34;</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>p</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># 获取损失相对于p的梯度</span>
</span></span><span class=line><span class=cl>                <span class=n>g</span> <span class=o>=</span> <span class=n>p</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>data</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># 更新一、二阶矩估计</span>
</span></span><span class=line><span class=cl>                <span class=n>m</span> <span class=o>=</span> <span class=n>beta1</span> <span class=o>*</span> <span class=n>m</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>beta1</span><span class=p>)</span> <span class=o>*</span> <span class=n>g</span>
</span></span><span class=line><span class=cl>                <span class=n>v</span> <span class=o>=</span> <span class=n>beat2</span> <span class=o>*</span> <span class=n>v</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>beat2</span><span class=p>)</span> <span class=o>*</span> <span class=n>g</span> <span class=o>*</span> <span class=n>g</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># 计算当前迭代的调整后学习率 αt</span>
</span></span><span class=line><span class=cl>                <span class=n>alpha_t</span> <span class=o>=</span> <span class=n>alpha</span> <span class=o>*</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>beat2</span><span class=o>**</span><span class=n>t</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>beta1</span><span class=o>**</span><span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=c1># 更新参数</span>
</span></span><span class=line><span class=cl>                <span class=n>p</span><span class=o>.</span><span class=n>data</span> <span class=o>-=</span> <span class=n>alpha_t</span> <span class=o>*</span> <span class=n>m</span> <span class=o>/</span> <span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>v</span><span class=p>)</span> <span class=o>+</span> <span class=n>eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=c1># 应用权重衰减</span>
</span></span><span class=line><span class=cl>                <span class=n>p</span><span class=o>.</span><span class=n>data</span> <span class=o>-=</span> <span class=n>alpha</span> <span class=o>*</span> <span class=n>weight_decay</span> <span class=o>*</span> <span class=n>p</span><span class=o>.</span><span class=n>data</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># 递增迭代次数</span>
</span></span><span class=line><span class=cl>                <span class=n>state</span><span class=p>[</span><span class=s2>&#34;t&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>t</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>                <span class=n>state</span><span class=p>[</span><span class=s2>&#34;m&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>m</span>
</span></span><span class=line><span class=cl>                <span class=n>state</span><span class=p>[</span><span class=s2>&#34;v&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>v</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>loss</span></span></span></code></pre></div></div><p><strong>Problem（AdamW Accounting）：AdamW 训练的资源核算</strong></p><p>计算运行 AdamW 所需的内存和计算资源。假设所有张量都使用 float32 精度。</p><p><strong>(a) 运行 AdamW 需要多少峰值内存？</strong></p><p>根据参数、激活值、梯度和优化器状态的内存使用情况分解答案。用批量大小（batch_size）和模型超参数（vocab_size、context_length、num_layers、d_model、num_heads）表示。假设 $d_{ff} = 4 \times d_{model}$。</p><p>为简化计算，激活值的内存使用仅考虑以下组件：</p><ul><li>Transformer 块<ul><li>RMSNorm 层</li><li>多头自注意力子层：QKV 投影、$Q^{\top}K$ 矩阵乘法、softmax、值的加权和、输出投影</li><li>位置前馈网络：$W_1$ 矩阵乘法、SiLU 激活、$W_2$ 矩阵乘法</li></ul></li><li>最终的 RMSNorm</li><li>输出嵌入</li><li>对数几率的交叉熵计算</li></ul><p>分别给出参数、激活值、梯度和优化器状态的代数表达式，以及总内存的代数表达式。</p><ul><li><strong>参数内存</strong>：</li></ul><p>$$
M_{\text{params}} = 4(2Vd + L(16d^2 + 2d) + d)
$$</p><p>其中 $V$ 为词表大小，$d$ 为模型维度，$L$ 为层数。</p><ul><li><strong>梯度内存</strong>：</li></ul><p>$$
M_{\text{grad}} = 4(2Vd + L(16d^2 + 2d) + d)
$$</p><ul><li><strong>优化器状态内存</strong>（AdamW 存储一阶矩和二阶矩）：</li></ul><p>$$
M_{\text{opt}} = 8(2Vd + L(16d^2 + 2d) + d)
$$</p><ul><li><strong>激活内存</strong>（基于中间张量的保守估计）：</li></ul><p>$$
M_{\text{act}} = 4[L(16BTd + 2BhT^2) + BTd + 2BTV]
$$</p><p>其中 $B$ 为批次大小，$T$ 为上下文长度，$h$ 为注意力头数。</p><ul><li><strong>总峰值内存</strong>：</li></ul><p>$$
M_{\text{total}} = 16(2Vd + L(16d^2 + 2d) + d) + 4[L(16BTd + 2BhT^2) + BTd + 2BTV]
$$</p><p>参考 DeepSeek 给的思路，给出激活内存的计算过程。</p><p><strong>激活内存计算过程</strong></p><p>激活内存指前向传播中需要存储的中间变量，用于反向传播计算梯度。根据给定的组件，我们逐项计算每个组件的激活内存（以元素数量计）：</p><p><strong>1. Transformer 块（共 L 层）</strong></p><p>每个 Transformer 层包含以下部分，其激活内存计算如下：</p><ul><li><strong>RMSNorm(s)</strong>:<br>输入和输出形状均为 <code>[B, T, d]</code>，需要存储输出（输入通常来自上一层已存储）。按存储输出计算：<code>B × T × d</code>。<br>每层有两个 RMSNorm，共 <code>2 × B × T × d</code>。</li><li><strong>多头自注意力子层</strong>:<ul><li><strong>QKV 投影</strong>：通过线性层将输入 <code>[B, T, d]</code> 投影为 Q、K、V。通常合并为一个输出张量，形状 <code>[B, T, 3d]</code>，然后拆分为三个独立的 <code>[B, T, d]</code> 张量。需存储这三个张量，共 <code>3 × B × T × d</code>。</li><li><strong>Q⊤K 矩阵乘法</strong>：计算注意力分数，输出形状 <code>[B, h, T, T]</code>，需存储。元素数量为 <code>B × h × T × T</code>。</li><li><strong>softmax</strong>：对注意力分数进行归一化，输出形状与输入相同，需存储。元素数量为 <code>B × h × T × T</code>。</li><li><strong>加权和</strong>：将注意力权重与 V 相乘，得到每个头的输出，然后合并多头，输出形状 <code>[B, T, d]</code>，需存储。元素数量为 <code>B × T × d</code>。</li><li><strong>输出投影</strong>：线性层，输入 <code>[B, T, d]</code>，输出 <code>[B, T, d]</code>，需存储。元素数量为 <code>B × T × d</code>。</li></ul></li><li><strong>位置前馈网络</strong>:<ul><li><strong>W1 矩阵乘法</strong>：将输入 <code>[B, T, d]</code> 投影到 <code>[B, T, 4d]</code>（因 <code>d_ff = 4 × d</code>），需存储。元素数量为 <code>4 × B × T × d</code>。</li><li><strong>SiLU 激活</strong>：输入和输出均为 <code>[B, T, 4d]</code>，需存储输出。元素数量为 <code>4 × B × T × d</code>。</li><li><del><strong>W3 矩阵乘法</strong>：将输入 <code>[B, T, d]</code> 投影到 <code>[B, T, 4d]</code>（因 <code>d_ff = 4 × d</code>），需存储。元素数量为 <code>4 × B × T × d</code>。</del></li><li><del><strong>GLU门控</strong>：输入和输出<code>[B, T, 4d]</code>，需存储输出。元素数量为 <code>4 × B × T × d</code>。</del></li><li><strong>W2 矩阵乘法</strong>：将 <code>[B, T, 4d]</code> 投影回 <code>[B, T, d]</code>，需存储。元素数量为 <code>B × T × d</code>。</li></ul></li></ul><p>此外，每个 Transformer 层还有残差连接，需要存储层的输入（来自上一层输出）和最终输出。但这些通常已在其他组件中考虑或可重用，不单独计算。</p><p>汇总一个 Transformer 层的激活内存元素数量：</p><p>$$
(2 + 3 + 1 + 1 + 4 + 4 + 1) \times BTd + 2BhT^2 = 16BTd + 2BhT^2
$$</p><p>故根据题目要求，我们采用给定表达式：</p><p>$$
\text{每层激活内存（元素）} = 16BTd + 2BhT^2
$$
<strong>2. 其他组件</strong></p><ul><li><strong>最终 RMSNorm</strong>：输入和输出形状 <code>[B, T, d]</code>，需存储输出。元素数量为 <code>B × T × d</code>。</li><li><strong>输出嵌入</strong>：将隐藏状态投影到词表，输出 logits 形状 <code>[B, T, V]</code>，需存储。元素数量为 <code>B × T × V</code>。</li><li><strong>交叉熵损失</strong>：需要 logits 计算损失，logits 已存储；需要计算 log_softmax，临时存储probs<code>B × T × V</code>；标签通常为整数，不占显著激活内存。</li></ul><p><strong>3. 总激活内存</strong></p><p>综合以上，总激活内存元素数量为：</p><p>$$
L(16BTd + 2BhT^2) + BTd + 2BTV
$$</p><p>转换为字节（乘以 4，因 float32 占 4 字节）：</p><p>$$
M_{\text{act}} = 4[L(16BTd + 2BhT^2) + BTd + 2BTV]
$$
<strong>(b) 针对 GPT-2 XL 规模的模型实例化答案，得到仅依赖于批量大小（batch_size）的表达式。在 80GB 内存中，最多可使用多大的批量大小？</strong></p><p>给出形如 $a \times B + b$ 的表达式（其中 a、b 为数值），以及最大批量大小的数值。</p><p>总内存：</p><p>$$
M_{\text{total}} \approx 14.45B + 31.70 \text{ GB}
$$</p><p>设内存上限为 80 GB：</p><p>$$
14.45B + 31.70 \leq 80 \implies B \leq \frac{80 - 31.70}{14.45} \approx 3.34
$$</p><blockquote><p><strong>答案</strong>：最大批次大小为 <strong>3</strong>。</p></blockquote><p><strong>(c) 执行一次 AdamW 步骤需要多少 FLOPs？</strong></p><p>给出代数表达式，并简要说明理由。</p><p>GPT-2 XL 参数总数 P = 2,127,057,600</p><p>AdamW 更新每个参数约需 10 次浮点操作：</p><ol><li>计算一阶矩估计: 2 FLOPs (乘法和加法)</li><li>计算二阶矩估计: 2 FLOPs (乘法和加法)</li><li>偏差修正: 2 FLOPs (幂运算和除法)</li><li>参数更新: 4 FLOPs (除法、平方根、乘法、减法)
总计: ~10 FLOPs/参数</li></ol><p>一步 AdamW 的总 FLOPs = 10 × P = 2.13e+10 次</p><p><strong>(d) 模型 FLOPs 利用率（MFU）定义为观测吞吐量（每秒处理的 token 数）与硬件理论峰值 FLOP 吞吐量的比值。NVIDIA A100 GPU 的 float32 运算理论峰值为 19.5 太拉 FLOP/s。假设 MFU 为 50%，在单个 A100 上，使用批量大小 1024 训练 GPT-2 XL 400K 步骤需要多长时间？按照 Kaplan 等人和 Hoffmann 等人的假设，反向传播的 FLOPs 是前向传播的两倍。</strong></p><p>给出训练所需的天数，并简要说明理由。</p><p>每训练步的 FLOPs 主要来自前向与后向传播。假设：</p><ul><li>前向传播 FLOPs 约为 <code>2 × B × T × P</code>（每个参数一次乘加，即 2 FLOPs）。</li><li>后向传播 FLOPs 约为前向的 2 倍。</li></ul><p>每步总 FLOPs：</p><p>$$
\text{FLOPs}_{\text{step}} \approx 6 \times B \times T \times P
$$</p><p>代入 $B = 1024$，$T = 1024$，$P = 2,127,057,600$：</p><p>$$
\text{FLOPs}_{\text{step}} \approx 6 \times 1024 \times 1024 \times 2.13 \times 10^9 \approx 1.34 \times 10^{16}
$$</p><p>总步数 400k，总 FLOPs：</p><p>$$
\text{FLOPs}_{\text{total}} \approx 400,000 \times 1.34 \times 10^{16} = 5.35 \times 10^{21}
$$</p><p>NVIDIA A100 峰值吞吐为 $19.5$ TFLOPS，50% MFU 下实际吞吐：</p><p>$$
\text{Throughput} = 0.5 \times 1.95 \times 10^{13} = 9.75 \times 10^{12} \text{ FLOP/s}
$$</p><p>训练时间：</p><p>$$
t = \frac{5.35 \times 10^{21}}{9.75 \times 10^{12}} \approx 5.5 \times 10^8 \text{ 秒} \approx 6354 \text{ 天} \approx 17.4 \text{ 年}
$$</p><blockquote><p><strong>答案</strong>：在单 A100 上以 50% MFU 训练需约 <strong>17.4 年</strong>。</p></blockquote></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 2026-01-30&nbsp;<a class=git-hash href=https://github.com/dillonzq/LoveIt/commit/5d3b6144d8ac2d85745c93a1575b2f1c567d6339 target=_blank title="commit by Koschei(nitianzero@gmail.com) 5d3b6144d8ac2d85745c93a1575b2f1c567d6339: chore: update cs336-assign1">
<i class="fas fa-hashtag fa-fw" aria-hidden=true></i>5d3b614</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-markdown href=/cs336-assign1/index.md target=_blank>阅读原始文档</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 X" data-sharer=x data-url=https://koschei.top/cs336-assign1/ data-title="[斯坦福CS336] 作业一：构建 Transformer 语言模型" data-hashtags=CS336,LLM><i class="fab fa-x-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://koschei.top/cs336-assign1/ data-hashtag=CS336><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="分享到 Hacker News" data-sharer=hackernews data-url=https://koschei.top/cs336-assign1/ data-title="[斯坦福CS336] 作业一：构建 Transformer 语言模型"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="分享到 Line" data-sharer=line data-url=https://koschei.top/cs336-assign1/ data-title="[斯坦福CS336] 作业一：构建 Transformer 语言模型"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@14.9.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://koschei.top/cs336-assign1/ data-title="[斯坦福CS336] 作业一：构建 Transformer 语言模型" data-ralateuid=xxxx><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/cs336/>CS336</a>,&nbsp;<a href=/tags/llm/>LLM</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/bpe-optimization/ class=prev rel=prev title="BPE 分词器高性能优化：从 10 分钟到 1 秒的实践"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>BPE 分词器高性能优化：从 10 分钟到 1 秒的实践</a>
<a href=/cs336-assign5/ class=next rel=next title=[斯坦福CS336]作业五：对齐与推理强化学习>[斯坦福CS336]作业五：对齐与推理强化学习<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=twikoo class=comment></div><script src=https://cdn.jsdelivr.net/npm/twikoo@1.6.44/dist/twikoo.all.min.js></script><script>twikoo.init({envId:"https://koschei.netlify.app/.netlify/functions/twikoo",el:"#twikoo",path:location.pathname,lang:"zh-CN"});const updateTwikooTheme=()=>{const e=document.body.getAttribute("theme")==="dark";document.documentElement.setAttribute("data-theme",e?"dark":"light")};updateTwikooTheme();const observer=new MutationObserver(e=>{e.forEach(e=>{e.attributeName==="theme"&&updateTwikooTheme()})});observer.observe(document.body,{attributes:!0})</script><noscript>请启用 JavaScript 以查看由 <a href=https://twikoo.js.org/ rel="nofollow noopener noreferrer" target=_blank>Twikoo</a> 提供的评论。</noscript></div></article></div></main><footer class=footer><div class=footer-container><div id=run-time data-start-time=2025-03-04T20:39:00+08:00 data-i18n-runtime="本站已运行 <i class='far fa-clock fa-fw'></i> %%DAYS%% 天 %%HOURS%% 小时 %%MINUTES%% 分钟 %%SECONDS%% 秒"></div><div class=footer-line>由 <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.155.1">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.3.0"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2025 - 2026</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://github.com/kosthi target=_blank>Koschei</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span><span class=icp-splitter>&nbsp;|&nbsp;</span><br class=icp-br><span class=icp><a href="https://beian.mps.gov.cn/#/query/webSearch?code=33032602100584" rel=noreferrer target=_blank><img src=/备案图标.png alt=备案图标 style=vertical-align:middle;width:16px>浙公网安备33032602100584号</a></span><span class=icp-splitter>&nbsp;|&nbsp;</span><br class=icp-br><span class=icp><a href=https://beian.miit.gov.cn/ rel=noreferrer target=_blank>浙ICP备2024075396号-2</a></span></div><div class=busuanzi-footer><span id=busuanzi_container_site_pv><i class="fa fa-eye"></i> <span id=busuanzi_value_site_pv></span>
</span>&nbsp;|&nbsp;
<span id=busuanzi_container_site_uv><i class="fa fa-user"></i> <span id=busuanzi_value_site_uv></span></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title=回到顶部><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i></a></div><div id=fixed-buttons-hidden><a href=# id=view-comments class=fixed-button title=查看评论><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css><script src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@5.20.2/dist/lite/builds/browser.umd.min.js></script><script src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js></script><script src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js></script><script src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js></script><script src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.2/sharer.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/copy-tex.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/mhchem.min.js></script><script>window.config={lightgallery:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{algoliaAppID:"GZT24PWKNT",algoliaIndex:"index",algoliaSearchKey:"634ab679e825b815de2663de38908f9d",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"}}</script><script src=/js/theme.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/jquery@2.1.3/dist/jquery.min.js></script><script type=text/javascript src=/js/custom.js></script></body></html>