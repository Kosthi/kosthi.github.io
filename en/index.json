[{"categories":["LLM"],"content":"1 Assignment Overview In this assignment, you will gain hands-on experience in training language models to reason when solving math problems. ","date":"2026-01-13","objectID":"/en/cs336-assign2/:1:0","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning","uri":"/en/cs336-assign2/"},{"categories":["LLM"],"content":"What to Implement Implement a zero-shot prompting baseline for the MATH competition dataset proposed by Hendrycks et al. [2021]. Supervised Fine-Tuning (SFT) using reasoning traces from a stronger reasoning model (DeepSeek R1, DeepSeekAI et al., 2025). Use Expert Iteration to improve reasoning performance through verification rewards. Use Group Relative Policy Optimization (GRPO) to improve reasoning performance through verification rewards. For interested students, we will release an optional part of the assignment in the coming days: aligning language models with human preferences. ","date":"2026-01-13","objectID":"/en/cs336-assign2/:1:1","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning","uri":"/en/cs336-assign2/"},{"categories":["LLM"],"content":"What to Run Evaluate the zero-shot prompting performance of the Qwen 2.5 Math 1.5B model (as a baseline). Supervised fine-tuning of Qwen 2.5 Math 1.5B using R1‚Äôs reasoning traces. Expert iteration training of Qwen 2.5 Math 1.5B using verification rewards. GRPO training of Qwen 2.5 Math 1.5B using verification rewards. Assignment link: Assignment5-alignment GitHub Repository Next, I will share some details and insights from completing the assignment. ","date":"2026-01-13","objectID":"/en/cs336-assign2/:1:2","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning","uri":"/en/cs336-assign2/"},{"categories":["LLM"],"content":"2 Reasoning Capabilities of Language Models ","date":"2026-01-13","objectID":"/en/cs336-assign2/:2:0","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning","uri":"/en/cs336-assign2/"},{"categories":["LLM"],"content":"2.1 Motivation One prominent application of language models is building general-purpose systems that can handle various natural language processing tasks. This assignment focuses on an emerging application: mathematical reasoning. We will use this as a testbed to build evaluation systems, perform supervised fine-tuning, and explore methods for training language models to reason using reinforcement learning (RL). This assignment differs from previous ones in two ways: We no longer use the language model codebase and models from previous assignments. Ideally, we would use the base language models trained in previous assignments, but fine-tuning those models won‚Äôt yield satisfactory results‚Äîthey are too weak to demonstrate complex mathematical reasoning. Therefore, we switch to an accessible, modern, high-performance language model (Qwen 2.5 Math 1.5B Base) for most of our work. We introduce a new benchmark dataset to evaluate language models. Previously, we considered cross-entropy as a good proxy for many downstream tasks. However, the core of this assignment is narrowing the gap between base models and downstream tasks, so we must use evaluation methods independent of cross-entropy. We will use the MATH 12K dataset proposed by Hendrycks et al. [2021], which contains challenging high school competition math problems. We evaluate language model performance by comparing model outputs against reference answers. ","date":"2026-01-13","objectID":"/en/cs336-assign2/:2:1","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning","uri":"/en/cs336-assign2/"},{"categories":["LLM"],"content":"2.2 Chain-of-Thought Reasoning and Reasoning Reinforcement Learning A recent hot trend in language models is using Chain-of-Thought (CoT) reasoning to improve performance on various tasks. Chain-of-thought refers to the process of reasoning through a problem step by step, generating intermediate reasoning steps before arriving at the final answer. Chain-of-Thought Reasoning in Language Models Early chain-of-thought methods decomposed problems into intermediate steps using a ‚Äúscratchpad‚Äù and fine-tuned language models to solve simple math tasks like arithmetic [Nye et al., 2021]. Other research prompted strong models to ‚Äúthink step by step‚Äù before answering, finding this significantly improved performance on math reasoning tasks like elementary arithmetic [Wei et al., 2023]. Learning Reasoning via Expert Iteration Self-Taught Reasoner (STaR) [Zelikman et al., 2022] constructs reasoning as a bootstrapping loop: the pretrained model first generates diverse chains of thought (CoTs), retaining only those that produce correct answers, then fine-tuning on these ‚Äúexpert‚Äù trajectories. Repeating this cycle improves the language model‚Äôs reasoning ability and problem-solving rate. STaR demonstrated that this expert iteration approach [Anthony et al., 2017] can bootstrap reasoning capabilities through automatic string matching verification without human-written reasoning trajectories. Reasoning Reinforcement Learning with Verification Rewards (o1, R1) Recent research explores using more powerful reinforcement learning algorithms combined with verification rewards to improve reasoning performance. OpenAI‚Äôs o1 (and subsequent o3/o4) [OpenAI et al., 2024], DeepSeek‚Äôs R1 [DeepSeek-AI et al., 2025], and Moonshot‚Äôs kimi k1.5 [Team et al., 2025] all use policy gradient methods [Sutton et al., 1999] to train on math and code tasks, verifying answer correctness through string matching or unit tests, achieving significant performance improvements on competition math and programming tasks. Follow-up work like Open-R1 [Face, 2025], SimpleRL-Zoo [Zeng et al., 2025], and TinyZero [Pan et al., 2025] demonstrate that pure reinforcement learning with verification rewards can improve reasoning performance even on models with only 1.5B parameters. Experimental Setup: Models and Datasets In the following sections, we will progressively adopt more complex methods to train base language models to solve math problems through step-by-step reasoning. This assignment uses the Qwen 2.5 Math 1.5B Base model, which is based on the Qwen 2.5 1.5B model and underwent continued pretraining on high-quality synthetic math pretraining data [Yang et al., 2024]. The MATH dataset is available at /data/a5-alignment/MATH on the Together cluster. Open Source Alternative Datasets (For Open Source Auditors) Due to copyright restrictions, the MATH dataset is not publicly available. If you‚Äôre completing the assignment locally, you can use the following open-source math reasoning datasets: Countdown [Pan et al., 2025]: A simple synthetic task based on the UK TV show ‚ÄúCountdown‚Äù, commonly used as a testbed for small-scale reasoning RL. GSM8K [Cobbe et al., 2021a]: Elementary arithmetic problem dataset, easier than MATH, suitable for debugging code correctness and familiarizing with reasoning RL workflow. Tulu 3 SFT Math [Lambert et al., 2025]: Synthetic math problems generated using GPT-4o and Claude 3.5 Sonnet. Since it‚Äôs synthetic data, some answers (or even questions) may be incorrect. Other math SFT datasets available online. If the dataset doesn‚Äôt directly provide short ground truth labels (like 1/2), you can use math answer parsers like Math-Verify to process the ground truth label column. ","date":"2026-01-13","objectID":"/en/cs336-assign2/:2:2","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning","uri":"/en/cs336-assign2/"},{"categories":["LLM"],"content":"3 Evaluating Zero-Shot MATH Performance We first evaluate the base language model‚Äôs performance on the MATH dataset‚Äôs 5K test set. Establishing this baseline helps understand how subsequent methods affect model behavior. ","date":"2026-01-13","objectID":"/en/cs336-assign2/:3:0","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning","uri":"/en/cs336-assign2/"},{"categories":["LLM"],"content":"Problem (math_baseline_performance): 4 points (a) Write a script to evaluate Qwen 2.5 Math 1.5B model‚Äôs zero-shot performance on the MATH dataset The script should complete the following tasks: Load MATH validation samples from /data/a5-alignment/MATH/validation.jsonl; Format these samples into string prompts acceptable by language models using the r1_zero prompt; Generate model outputs for each sample; Calculate evaluation metrics; Serialize samples, model generations, and corresponding evaluation scores to disk for subsequent analysis. During implementation, writing an evaluate_vllm method like below will be helpful, which you can reuse later: def evaluate_vllm( vllm_model: LLM, reward_fn: Callable[[str, str], dict[str, float]], prompts: List[str], eval_sampling_params: SamplingParams ) -\u003e None: \"\"\" Evaluate language model performance on a set of prompts, calculate evaluation metrics, and serialize results to disk. \"\"\"Submission: A script for evaluating zero-shot MATH dataset baseline performance. See at eval.py (b) Run your evaluation script on the Qwen 2.5 Math 1.5B model Count the number of model generations falling into each of the following three categories: Both format reward and answer reward are 1 (completely correct); Format reward is 1, answer reward is 0 (format correct, answer wrong); Both format reward and answer reward are 0 (both format and answer wrong). Observe at least 10 cases with format reward 0. Do you think the problem lies with the base model‚Äôs output or the parser? Why? Then observe at least 10 cases with format reward 1 but answer reward 0. What are your thoughts? Submission: Analysis of model and reward function performance, including examples of each category. 1. Count Statistics Based on the summary at the beginning of the log (num_examples=5000) and category counts, the model generation distribution is: Both format reward and answer reward are 1 (completely correct): count(fmt=1,ans=1)=139 cases. Format reward is 1, answer reward is 0 (format correct, answer wrong): count(fmt=1,ans=0)=693 cases. Both format reward and answer reward are 0 (both wrong): count(fmt=0,ans=0)=4168 cases. 2. Observations on ‚ÄúFormat Reward 0‚Äù Cases In at least 10 such cases from the log (marked as sample_fmt0_ans0_examples=10), the problem mainly lies with the base model‚Äôs output, not the parser. Main Reason: The common issue in these failure cases is that the model did not follow the specified output format. According to the log, the correct format should include clear \u003cthink\u003e reasoning and a final answer wrapped in \u003canswer\u003e tags. However, problematic model outputs often show: Confused format structure: For example, when answering ‚ÄúWhat is the positive difference between $120%$ of 30 and $130%$ of 20?‚Äù, the model gave the answer directly within \u003cthink\u003e tags and mixed in undefined tags like \u003cend think\u003e \u003canswer\u003e, causing parsing failure. Irrelevant content and symbols: For example, when answering the inverse function problem ‚ÄúLet $f(x)=7x+5$‚Ä¶‚Äù, the \u003cthink\u003e content contained lots of irrelevant code snippets, garbled characters, and even image links, completely deviating from mathematical reasoning. Reasoning and answer not separated: The model often wrote ‚Äúthe answer is‚Ä¶‚Äù in the thinking part, or the final answer wasn‚Äôt wrapped in required tags. These phenomena indicate that the base model has difficulty following strict structured output instructions and fails to generate text that parsers can correctly process. 3. Observations on ‚ÄúFormat Reward 1 but Answer Reward 0‚Äù Cases In at least 10 such cases from the log (marked as sample_fmt1_ans0_examples=10), the model successfully followed format requirements, but the answer itself was incorrect. Main Observations: This reveals the model‚Äôs core capability limitations. With correct formatting, error types include: Mathematical calculation errors: For example, calculating $i^5+i^{-25}+i^{45}$, the model got $2i$, but the correct answer should be $i$. Logical reasoning errors: F","date":"2026-01-13","objectID":"/en/cs336-assign2/:3:1","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning","uri":"/en/cs336-assign2/"},{"categories":["LLM"],"content":"4 Supervised Fine-Tuning for MATH ","date":"2026-01-13","objectID":"/en/cs336-assign2/:4:0","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning","uri":"/en/cs336-assign2/"},{"categories":["LLM"],"content":"Problem (sft_experiment): Run SFT on MATH dataset (2 points) (2 H100 hours) 1. Using Qwen 2.5 Math 1.5B base model, run SFT on reasoning SFT examples (provided in /data/a5-alignment/MATH/sft.jsonl), varying the number of unique examples in SFT in {128, 256, 512, 1024}, and using the full dataset. Tune learning rate and batch size to achieve at least 15% validation accuracy when using the full dataset. Submission: Validation accuracy curves associated with different dataset sizes. Fewer SFT samples actually work better than more samples. Entropy also drops faster and lower. With more samples, the model becomes confused instead. 2. Filter reasoning SFT examples to include only those producing correct answers. Run SFT on the (full) filtered dataset and report the filtered dataset size and validation accuracy achieved. Submission: Report dataset size and validation accuracy curve. Compare your findings with previous SFT experiments. Experiment pending ","date":"2026-01-13","objectID":"/en/cs336-assign2/:4:1","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning","uri":"/en/cs336-assign2/"},{"categories":["LLM"],"content":"5 Expert Iteration for MATH ","date":"2026-01-13","objectID":"/en/cs336-assign2/:5:0","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning","uri":"/en/cs336-assign2/"},{"categories":["LLM"],"content":"Problem (expert_iteration_experiment): Run Expert Iteration on MATH dataset (2 points) (6 H100 hours) Run expert iteration on the MATH dataset (provided in /data/a5-alignment/MATH/train.jsonl) using Qwen 2.5 Math 1.5B Base model, varying the number of rollouts per question G and number of epochs used in SFT step, using n_ei_steps = 5. Vary batch size for each expert iteration step (i.e., size of Db) in {512, 1024, 2048}. (You don‚Äôt need to try all possible combinations of these hyperparameters. Just enough to draw conclusions about each.) Record entropy of model responses during training. Make sure vLLM terminates generation at the second answer tag as done in SFT section. Submissions: Validation accuracy curves associated with different rollout configurations. Try at least 2 different rollout counts and epoch counts. A model achieving at least 15% validation accuracy on MATH. A short 2-sentence discussion comparing your performance with SFT performance, as well as performance across EI steps. A plot showing entropy of model responses during training. Experiment pending ","date":"2026-01-13","objectID":"/en/cs336-assign2/:5:1","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning","uri":"/en/cs336-assign2/"},{"categories":["LLM"],"content":"6 Policy Gradient Primer ","date":"2026-01-13","objectID":"/en/cs336-assign2/:6:0","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning","uri":"/en/cs336-assign2/"},{"categories":["LLM"],"content":"Why Should Systems Enthusiasts Learn Large Language Models? In today‚Äôs AI technology wave, mastering large model knowledge has become an essential skill for systems developers. By participating in Stanford CS336 Large Model Systems Course, I began my journey of building large models from scratch. This course is likely to become a landmark course in the systems field over the next 3 years (similar to the position of CMU 15-445 database course in recent years). ","date":"2025-06-14","objectID":"/en/cs336-assign1/:1:0","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 1: Building a Transformer Language Model","uri":"/en/cs336-assign1/"},{"categories":["LLM"],"content":"Assignment Overview This assignment implements a small language model through the following three modules: Tokenizer Design and Implementation ‚Äî Byte Pair Encoding (BPE) tokenizer Model Architecture Coding ‚Äî Transformer with Self-Attention mechanism Optimizer Development ‚Äî AdamW optimizer Assignment link: Assignment1-Basics GitHub Repository Next, I will share some details and insights from completing the assignment. ","date":"2025-06-14","objectID":"/en/cs336-assign1/:1:1","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 1: Building a Transformer Language Model","uri":"/en/cs336-assign1/"},{"categories":["LLM"],"content":"1. Byte Pair Encoding (BPE) Tokenizer ","date":"2025-06-14","objectID":"/en/cs336-assign1/:2:0","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 1: Building a Transformer Language Model","uri":"/en/cs336-assign1/"},{"categories":["LLM"],"content":"1.1 Unicode Standard Problem (unicode1): Understanding Unicode (1 point) (a) What Unicode character does chr(0) return? The NULL character, i.e., ASCII null character. (b) How does the string representation (‚Äôrepr()‚Äô‚Äô) differ from its printed representation? The repr() function displays the escape sequence, while printing displays nothing (empty character). (c) What happens when this character appears in text? Although the null character is invisible when printed, it still exists as part of the Python string. This shows that Python strings can contain invisible characters, and these null characters still affect string storage and processing. ","date":"2025-06-14","objectID":"/en/cs336-assign1/:2:1","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 1: Building a Transformer Language Model","uri":"/en/cs336-assign1/"},{"categories":["LLM"],"content":"1.2 Unicode Encoding Problem (unicode2): Unicode Encoding (1 point) (a) Why do we prefer training tokenizers on UTF-8 encoded bytes rather than UTF-16 or UTF-32? When training tokenizers, we process byte sequences. UTF-8 represents common characters more compactly, reducing sequence length, which is more efficient for model training. Moreover, UTF-8 is backward compatible with ASCII, making it especially efficient for processing English text. (b) Why is this decode function incorrect? This function is incorrect because it decodes byte by byte, which fails for multi-byte UTF-8 characters. (c) Give a two-byte sequence that cannot be decoded. 0x80 0x81 is invalid because in UTF-8, any byte starting with binary 10 must be a continuation byte, but here it appears as the first byte. ","date":"2025-06-14","objectID":"/en/cs336-assign1/:2:2","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 1: Building a Transformer Language Model","uri":"/en/cs336-assign1/"},{"categories":["LLM"],"content":"1.3 BPE Tokenizer Training Experiments BPE Optimization Strategies To pursue high performance, I used Pybind11 to bind C++ code: pre-tokenization is handled by Python, while the BPE merge process is delegated to C++. Main optimizations include: Parallelization ‚Äî Use OpenMP for parallel statistics, avoiding lock contention Lazy Deletion Queue ‚Äî Complexity reduced from O(NlogN) to O(KlogN) Incremental Updates ‚Äî Only update affected adjacent pairs Efficient Data Structures ‚Äî Integer IDs instead of strings, custom hash functions Performance Comparison (TinyStoriesV2-GPT4-train dataset): Version BPE Merge Training Speedup Python 10min++ Baseline C++ Unoptimized 366s ~2x C++ Optimized 1.08s 300x For detailed optimization principles and implementation, see: High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second Problem (train_bpe_tinystories): Train BPE on TinyStories (2 points) (a) How long did training take, and how much memory did it use? What is the longest token in the vocabulary? Is it reasonable? Training time: 28.03s Memory: 10GB Longest token: \" accomplishment\", ID: 7159, length: 15 characters (including leading space) Reasonable: Yes (b) Analyze code performance. Which part of tokenizer training takes the longest? N: Total distinct words L: Average word length V: Target vocabulary size M: Number of merges = V - 256 - |special_tokens| K: Occurrence count of specific pair P: Temporary pair frequency table Before optimization, the BPE merge process takes the longest (6 minutes), with time complexity O(M √ó N √ó L) and space complexity O(N √ó L + P). After optimization, it takes only about 1s, with time complexity O(N √ó L + M). Problem (train_bpe_expts_owt): Train BPE on OpenWebText (2 points) (a) Train a byte-level BPE tokenizer on OpenWebText with max vocabulary size 32,000. What is the longest token? Is it reasonable? Longest tokens: ID: 25835 | Byte length: 64 | Content: 64 hyphens ID: 25821 | Byte length: 64 | Content: Repeated UTF-8 sequences (b) Compare tokenizers trained on TinyStories vs OpenWebText. Some tokens contain newline characters, which when written to file without escaping, split a single merge rule across multiple lines. ","date":"2025-06-14","objectID":"/en/cs336-assign1/:2:3","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 1: Building a Transformer Language Model","uri":"/en/cs336-assign1/"},{"categories":["LLM"],"content":"1.4 Tokenizer Experiments Problem (tokenizer_experiments): Tokenizer Experiments (4 points) (a) Sample 10 documents each from TinyStories and OpenWebText. What is the compression ratio (bytes/token) for each tokenizer? TinyStories-10K tokenizer on TinyStories: 4.14 bytes/token OpenWebText-32K tokenizer on OpenWebText: 4.70 bytes/token (b) What happens when encoding OpenWebText with the TinyStories tokenizer? Using the TinyStories-10K tokenizer on OpenWebText documents, compression ratio drops to 3.26 bytes/token, indicating the smaller vocabulary (10K) produces more tokens for complex text. (c) Estimate tokenizer throughput. TinyStories-10K: ~626,519.6 bytes/sec, encoding 825GB Pile takes ~16.4 days OpenWebText-32K: ~763,734.4 bytes/sec, takes ~13.4 days (d) Why is uint16 an appropriate choice for token IDs? Both vocabularies (10K and 32K) are less than 65,536 (2^16), so uint16 suffices, saving 50% storage vs uint32. ","date":"2025-06-14","objectID":"/en/cs336-assign1/:2:4","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 1: Building a Transformer Language Model","uri":"/en/cs336-assign1/"},{"categories":["LLM"],"content":"2. Transformer Resource Accounting ","date":"2025-06-14","objectID":"/en/cs336-assign1/:3:0","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 1: Building a Transformer Language Model","uri":"/en/cs336-assign1/"},{"categories":["LLM"],"content":"2.1 FLOPs Accounting Basics Understanding the computational and memory footprint of Transformer components is useful. We will perform basic ‚ÄúFLOP accounting.‚Äù Most floating point operations in Transformers come from matrix multiplications, so the core idea is simple: List all matrix multiplication operations in Transformer forward pass Convert each to required FLOPs Matrix Multiplication FLOPs Rule: For matrices A ‚àà ‚Ñù^(m√ón) and B ‚àà ‚Ñù^(n√óp), the multiplication AB requires 2mnp FLOPs. ","date":"2025-06-14","objectID":"/en/cs336-assign1/:3:1","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 1: Building a Transformer Language Model","uri":"/en/cs336-assign1/"},{"categories":["LLM"],"content":"2.2 GPT-2 XL Resource Accounting Problem (transformer_accounting): Transformer LM Resource Accounting (5 points) (a) GPT-2 XL Trainable Parameters GPT-2 XL configuration: vocab_size: 50,257 context_length: 1,024 num_layers: 48 d_model: 1,600 num_heads: 25 d_ff: 6,400 Token Embedding Layer: 50,257 √ó 1,600 = 80,411,200 (~80M) Single Transformer Block: MHA (Q/K/V + Output projections): 4 √ó 1,600 √ó 1,600 = 10,240,000 (~10M) FFN (SwiGLU with W1, W2, W3): 3 √ó 6,400 √ó 1,600 = 30,720,000 (~31M) 2 RMSNorm layers: 2 √ó 1,600 = 3,200 Total per layer: 40,963,200 (~41M) 48 layers total: 1,966,233,600 (~1.97B) Final RMSNorm: 1,600 parameters Output Projection (LM Head): 50,257 √ó 1,600 = 80,411,200 (~80M) Total: ~2.13 billion parameters, requiring ~8 GB memory in float32. (b) List all matrix multiplications for forward pass FLOPs Operation Dimensions MHA Q/K/V Projections (seq, d_model) √ó (d_model, d_model) √ó 3 per layer Attention Score Computation (seq, d_k) √ó (d_k, seq) per head per layer Attention Weighted Sum (seq, seq) √ó (seq, d_v) per head per layer MHA Output Projection (seq, d_model) √ó (d_model, d_model) per layer FFN Layers (W1, W2, W3) (seq, d_model) √ó (d_model, d_ff) √ó 3 per layer Output Layer (LM Head) (seq, d_model) √ó (d_model, vocab_size) Total for seq_len = 1024: ~4.20 TFLOPs (c) Which parts consume most FLOPs? Attention Q/K/V projections: 17.96% Attention score/weighted sum: 0.30% Attention output projection: 5.99% FFN (all three layers): 71.83% Output layer: 3.92% FFN is the dominant component (~72% of FLOPs). (d) How do FLOPs distributions change with model scale? Component GPT-2 Small GPT-2 Medium GPT-2 Large GPT-2 XL MHA Q/K/V Projections 13.84% 16.51% 17.47% 17.96% Attention Score/Sum 1.02% 0.68% 0.46% 0.30% FFN (all layers) 55.36% 66.04% 69.89% 71.83% Output Layer 25.16% 11.25% 6.35% 3.92% Key trends: FFN proportion increases significantly with scale Attention score computation proportion decreases sharply Output layer proportion drops dramatically (e) What happens if context_length increases to 16,384? Total FLOPs increase from ~3.5T to ~70.4T (~20x increase, not linear due to O(S¬≤) attention) Attention module proportion increases from ~24% to ~28% FFN proportion decreases from ~72% to ~69% ","date":"2025-06-14","objectID":"/en/cs336-assign1/:3:2","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 1: Building a Transformer Language Model","uri":"/en/cs336-assign1/"},{"categories":["LLM"],"content":"3. Training Transformer Language Models ","date":"2025-06-14","objectID":"/en/cs336-assign1/:4:0","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 1: Building a Transformer Language Model","uri":"/en/cs336-assign1/"},{"categories":["LLM"],"content":"3.1 Cross-Entropy Loss The Transformer LM defines distribution p_Œ∏(x_{i+1} | x_{1:i}) for sequence positions. Cross-entropy (negative log-likelihood) loss: 85302 \\ell( heta; D) = rac{1}{|D|} \\sum_{x \\in D} \\sum_{i=1}^{m} -\\log p_{ heta}(x_{i+1} | x_{1:i}) 85302 Problem (cross_entropy): Implement Cross-Entropy Implemented using numerically stable log_softmax. ","date":"2025-06-14","objectID":"/en/cs336-assign1/:4:1","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 1: Building a Transformer Language Model","uri":"/en/cs336-assign1/"},{"categories":["LLM"],"content":"3.2 Learning Rate Tuning Problem (learning_rate_tuning): Tune Learning Rate Testing SGD with different learning rates for 10 iterations: LR = 10: Loss slowly decays LR = 100: Loss rapidly decays, converging near zero LR = 1000: Loss explodes, clearly diverging ","date":"2025-06-14","objectID":"/en/cs336-assign1/:4:2","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 1: Building a Transformer Language Model","uri":"/en/cs336-assign1/"},{"categories":["LLM"],"content":"3.3 Implementing AdamW Problem (adamw): Implement AdamW AdamW implementation with: First moment estimate (m) Second moment estimate (v) Bias correction Decoupled weight decay Problem (AdamW Accounting): AdamW Training Resource Accounting (a) Peak Memory for AdamW Parameters: 4(2Vd + L(16d¬≤ + 2d) + d) bytes Gradients: Same as parameters Optimizer state (m and v): 2√ó parameter memory Activations: 4[L(16BTd + 2BhT¬≤) + BTd + 2BTV] bytes (b) For GPT-2 XL, what is max batch size in 80GB? Total memory ‚âà 14.45B + 31.70 GB Maximum batch size: 3 (c) FLOPs for one AdamW step? ~10 FLOPs per parameter for: First moment update: 2 FLOPs Second moment update: 2 FLOPs Bias correction: 2 FLOPs Parameter update: 4 FLOPs Total: 10 √ó 2.13B = ~21.3 GFLOPs per step (d) Training time for GPT-2 XL on single A100? FLOPs per step: 6 √ó B √ó T √ó P ‚âà 1.34 √ó 10^16 Total FLOPs for 400K steps: ~5.35 √ó 10^21 A100 at 50% MFU: 9.75 √ó 10^12 FLOP/s Training time: ~17.4 years on single A100 ","date":"2025-06-14","objectID":"/en/cs336-assign1/:4:3","tags":["CS336","LLM"],"title":"[Stanford CS336] Assignment 1: Building a Transformer Language Model","uri":"/en/cs336-assign1/"},{"categories":["LLM"],"content":"Detailed explanation of high-performance optimization strategies for BPE tokenizers, including parallelization, lazy deletion queues, incremental updates, and more","date":"2025-06-14","objectID":"/en/bpe-optimization/","tags":["CS336","LLM","BPE","Performance Optimization"],"title":"High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second","uri":"/en/bpe-optimization/"},{"categories":["LLM"],"content":" This article is supplementary reading for CS336 Assignment 1, providing a detailed introduction to the optimized implementation of the BPE tokenizer. ","date":"2025-06-14","objectID":"/en/bpe-optimization/:0:0","tags":["CS336","LLM","BPE","Performance Optimization"],"title":"High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second","uri":"/en/bpe-optimization/"},{"categories":["LLM"],"content":"Background The recommended cppyy in the documentation has issues in Mac and Linux environments. To pursue high performance, I used Pybind11 to bind C++ code: pre-tokenization is handled by Python, while the BPE merge process is delegated to C++. The actual biggest bottleneck is still pre-tokenization, which can be parallelized using the existing code pretokenization_example.py for chunked parallel processing (8 cores 100s ‚Üí 16 cores 30s). ","date":"2025-06-14","objectID":"/en/bpe-optimization/:1:0","tags":["CS336","LLM","BPE","Performance Optimization"],"title":"High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second","uri":"/en/bpe-optimization/"},{"categories":["LLM"],"content":"Core Optimization Strategies ","date":"2025-06-14","objectID":"/en/bpe-optimization/:2:0","tags":["CS336","LLM","BPE","Performance Optimization"],"title":"High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second","uri":"/en/bpe-optimization/"},{"categories":["LLM"],"content":"1. Parallelization Use OpenMP to parallelize the build_initial_counts() function Each thread maintains local statistics (ThreadLocal structure) to avoid frequent lock contention Finally merge each thread‚Äôs results into global statistics ","date":"2025-06-14","objectID":"/en/bpe-optimization/:2:1","tags":["CS336","LLM","BPE","Performance Optimization"],"title":"High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second","uri":"/en/bpe-optimization/"},{"categories":["LLM"],"content":"2. Lazy Deletion Priority Queue Use a max heap (priority_queue) to quickly find the highest-frequency token pair Adopt a ‚Äúlazy deletion‚Äù strategy: don‚Äôt directly delete expired pairs from the queue When extracting a pair from the top of the queue, check if it‚Äôs still valid (whether the frequency matches current statistics) Complexity reduced from O(NlogN) to O(KlogN), where K is the number of expired entries to skip ","date":"2025-06-14","objectID":"/en/bpe-optimization/:2:2","tags":["CS336","LLM","BPE","Performance Optimization"],"title":"High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second","uri":"/en/bpe-optimization/"},{"categories":["LLM"],"content":"3. Incremental Update Mechanism The merge_and_update() function only updates affected adjacent pairs when merging tokens Maintain a pair_positions index structure to record where each pair appears in which words and at what positions Avoid rescanning all words after each merge, greatly reducing computation ","date":"2025-06-14","objectID":"/en/bpe-optimization/:2:3","tags":["CS336","LLM","BPE","Performance Optimization"],"title":"High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second","uri":"/en/bpe-optimization/"},{"categories":["LLM"],"content":"4. Efficient Data Structures Use integer IDs (0-255) to represent bytes, avoiding frequent string operations Custom hash function PairHash supports pairs as keys in unordered_map Use -1 to mark merged tokens, avoiding data movement ","date":"2025-06-14","objectID":"/en/bpe-optimization/:2:4","tags":["CS336","LLM","BPE","Performance Optimization"],"title":"High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second","uri":"/en/bpe-optimization/"},{"categories":["LLM"],"content":"5. Memory-Friendly Representation Words are stored as integer vectors instead of strings Vocabulary uses map\u003cint, Bytes\u003e, supporting fast ID to byte string lookup Special tokens are added at the end of training, not affecting the core training process ","date":"2025-06-14","objectID":"/en/bpe-optimization/:2:5","tags":["CS336","LLM","BPE","Performance Optimization"],"title":"High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second","uri":"/en/bpe-optimization/"},{"categories":["LLM"],"content":"6. Flexible Training Control Support specifying target vocabulary size Support special tokens (like \u003cpad\u003e) Return complete merge records for subsequent encoding use ","date":"2025-06-14","objectID":"/en/bpe-optimization/:2:6","tags":["CS336","LLM","BPE","Performance Optimization"],"title":"High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second","uri":"/en/bpe-optimization/"},{"categories":["LLM"],"content":"Detailed Example: Optimized Workflow Let‚Äôs use a concrete example to illustrate how these optimizations work together. ","date":"2025-06-14","objectID":"/en/bpe-optimization/:3:0","tags":["CS336","LLM","BPE","Performance Optimization"],"title":"High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second","uri":"/en/bpe-optimization/"},{"categories":["LLM"],"content":"Input Data words = {\"low\", \"lower\", \"widest\", \"newest\"} counts = [5, 2, 3, 6]Initial token frequency statistics (when frequencies are equal, compare by lexicographic order): Token Pair Frequency Source (‚Äúe‚Äù,‚Äús‚Äù) 9 newest(6) + widest(3) (‚Äús‚Äù,‚Äút‚Äù) 9 newest(6) + widest(3) (‚Äúw‚Äù,‚Äúe‚Äù) 8 newest(6) + lower(2) (‚Äúl‚Äù,‚Äúo‚Äù) 7 low(5) + lower(2) (‚Äúo‚Äù,‚Äúw‚Äù) 7 low(5) + lower(2) Lexicographic comparison when frequencies are equal: \"es\" corresponds to (‚Äúe‚Äù,‚Äús‚Äù) \"st\" corresponds to (‚Äús‚Äù,‚Äút‚Äù) Lexicographic comparison: \"es\" \u003c \"st\", so (‚Äúe‚Äù,‚Äús‚Äù) has lower priority than (‚Äús‚Äù,‚Äút‚Äù) In the max heap, lower priority items sink, so the top of the heap is (‚Äús‚Äù,‚Äút‚Äù). ","date":"2025-06-14","objectID":"/en/bpe-optimization/:3:1","tags":["CS336","LLM","BPE","Performance Optimization"],"title":"High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second","uri":"/en/bpe-optimization/"},{"categories":["LLM"],"content":"Lazy Deletion Queue Workflow First Merge: Merge (‚Äús‚Äù,‚Äút‚Äù) into new token 256 Initial queue state (max heap, top priority first): Top: (\"s\",\"t\"):9 \u003c-- will be merged (\"e\",\"s\"):9 (\"w\",\"e\"):8 (\"l\",\"o\"):7 (\"o\",\"w\"):7 Impact of merge operation: Word ‚Äúnewest‚Äù: [110,101,119,101,115,116] ‚Üí [110,101,119,101,256,-1] Word ‚Äúwidest‚Äù: [119,105,100,101,115,116] ‚Üí [119,105,100,101,256,-1] Incremental update (instead of recalculating everything): // For \"newest\": // Delete affected pairs: (\"e\",\"s\"):6, (\"s\",\"t\"):6 // Add new pairs: (\"e\",256):6 // For \"widest\": // Delete affected pairs: (\"e\",\"s\"):3, (\"s\",\"t\"):3 // Add new pairs: (\"e\",256):3 Queue update (lazy approach): Don‚Äôt immediately delete old entries from the queue Push new pairs into the queue: (\"e\",256):9 Queue now contains mixed old and new entries When getting the next highest frequency pair: while (!pair_queue.empty()) { best_info = pair_queue.top(); pair_queue.pop(); auto it = pair_counts.find(best_info.pair); if (it != pair_counts.end() \u0026\u0026 it-\u003esecond == best_info.count) { break; // Valid, use it } // Otherwise, this is an expired entry, continue checking next } ","date":"2025-06-14","objectID":"/en/bpe-optimization/:3:2","tags":["CS336","LLM","BPE","Performance Optimization"],"title":"High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second","uri":"/en/bpe-optimization/"},{"categories":["LLM"],"content":"Specific Numeric Changes in Incremental Update Global statistics before merge: pair_counts = { (\"e\",\"s\"):9, (\"s\",\"t\"):9, (\"w\",\"e\"):8, (\"l\",\"o\"):7, (\"o\",\"w\"):7, (\"n\",\"e\"):6, (\"e\",\"w\"):6, ... }Incremental update after merging (‚Äús‚Äù,‚Äút‚Äù): For ‚Äúnewest‚Äù (frequency 6): Delete left adjacent (‚Äúe‚Äù,‚Äús‚Äù): pair_counts[(\"e\",\"s\")] -= 6 ‚Üí from 9 to 3 Delete (‚Äús‚Äù,‚Äút‚Äù) itself: pair_counts[(\"s\",\"t\")] -= 6 ‚Üí from 9 to 3 Add new left adjacent (‚Äúe‚Äù,256): pair_counts[(\"e\",256)] += 6 ‚Üí from 0 to 6 For ‚Äúwidest‚Äù (frequency 3): Delete left adjacent (‚Äúe‚Äù,‚Äús‚Äù): pair_counts[(\"e\",\"s\")] -= 3 ‚Üí from 3 to 0 Delete (‚Äús‚Äù,‚Äút‚Äù) itself: pair_counts[(\"s\",\"t\")] -= 3 ‚Üí from 3 to 0 Add new left adjacent (‚Äúe‚Äù,256): pair_counts[(\"e\",256)] += 3 ‚Üí from 6 to 9 ","date":"2025-06-14","objectID":"/en/bpe-optimization/:3:3","tags":["CS336","LLM","BPE","Performance Optimization"],"title":"High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second","uri":"/en/bpe-optimization/"},{"categories":["LLM"],"content":"Advantages of Parallel Processing Suppose we have 8 threads processing 1 million words: Before optimization (serial): Single thread scans 1 million words, counting all adjacent pairs Time complexity: O(N√óM), where M is average word length After optimization (parallel): #pragma omp parallel for schedule(static) for (size_t i = 0; i \u003c 1000000; ++i) { // Each thread processes about 125,000 words // Thread-local statistics, no lock contention } // Finally merge thread-local results Performance improvement: Ideal case: 8 threads speedup ‚âà 6-7x Actual considering thread creation/merge overhead: speedup ‚âà 5-6x ","date":"2025-06-14","objectID":"/en/bpe-optimization/:3:4","tags":["CS336","LLM","BPE","Performance Optimization"],"title":"High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second","uri":"/en/bpe-optimization/"},{"categories":["LLM"],"content":"Memory Efficiency Comparison Method Store ‚Äúnewest‚Äù After merging ‚Äúst‚Äù Memory Usage String array [\"n\",\"e\",\"w\",\"e\",\"s\",\"t\"] [\"n\",\"e\",\"w\",\"e\",\"st\"] Requires moving/copying strings Integer ID+tag [110,101,119,101,115,116] [110,101,119,101,256,-1] Only modify two integers ","date":"2025-06-14","objectID":"/en/bpe-optimization/:3:5","tags":["CS336","LLM","BPE","Performance Optimization"],"title":"High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second","uri":"/en/bpe-optimization/"},{"categories":["LLM"],"content":"Performance Comparison Test machine: autodl Xeon(R) Platinum 8352V 32-core CPU with 60GB RAM, pre-tokenization uses 24 cores for parallel processing. Data Version Pre-tokenization BPE Merge Training Total Time TinyStoriesV2-GPT4-train Python 29.65s 10min++ Unacceptable TinyStoriesV2-GPT4-train Cpp unoptimized merge 27.337s 366.644s 394.16s TinyStoriesV2-GPT4-train Cpp optimized merge 26.767s 1.081s 28.03s TinyStoriesV2-GPT4-train Rust optimized pre-tokenization 67.261s - - Python‚Äôs regex library has excellent underlying C optimization. C++‚Äôs regex library has incomplete Unicode support, and Rust is actually twice as slow as Python. As the documentation states: \"‚Ä¶but the regex package in Python is, if anything, even faster.\" ","date":"2025-06-14","objectID":"/en/bpe-optimization/:4:0","tags":["CS336","LLM","BPE","Performance Optimization"],"title":"High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second","uri":"/en/bpe-optimization/"},{"categories":["LLM"],"content":"Summary These optimizations enable the algorithm to efficiently process large-scale text data, with particularly excellent performance in building initial statistics and iterative merge stages: Parallelization speeds up initial counting Lazy deletion priority queue reduces maintenance overhead Incremental update mechanism avoids unnecessary repeated calculations The final result is a performance improvement from over 10 minutes to about 1 second, a speedup of over 300x. ","date":"2025-06-14","objectID":"/en/bpe-optimization/:5:0","tags":["CS336","LLM","BPE","Performance Optimization"],"title":"High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second","uri":"/en/bpe-optimization/"},{"categories":["Guides"],"content":"Introduction Today, most platforms have gradually transitioned from ‚Äúone-time purchase‚Äù to ‚Äúsubscription-based‚Äù, and 1Password 8 is no exception. It‚Äôs understandable that companies make such moves to maintain operations and continue development. As of March 2025, 1Password 8 Individual costs USD $35.88 per year, and Teams Starter (10 users) costs USD $239.4 per year. Once you‚Äôre approved for 1Password for Open Source Project, you can get a permanent 1Password for Teams subscription for free, which is truly generous. When you see this paragraph on the GitHub page of 1Password for Open Source, you‚Äôll understand their intention. It‚Äôs fair to say that 1Password wouldn‚Äôt exist without the open source community, so we want to give back and help teams be more productive and secure. ","date":"2025-03-14","objectID":"/en/applying-1password-open-source-plan/:1:0","tags":["Apply","1Password","OpenSource"],"title":"Apply for 1Password OpenSource Plan Through Open Source Projects to Get Teams Subscription License","uri":"/en/applying-1password-open-source-plan/"},{"categories":["Guides"],"content":"Application Requirements 1Password‚Äôs requirements for open source projects are actually not strict. You need to meet one of the following requirements: You are a core contributor to an active open source project that has been created for at least 30 days You are an organizer of an open source community meetup/event/conference Additionally, your project must: Use a standard open source license Be non-commercial ","date":"2025-03-14","objectID":"/en/applying-1password-open-source-plan/:2:0","tags":["Apply","1Password","OpenSource"],"title":"Apply for 1Password OpenSource Plan Through Open Source Projects to Get Teams Subscription License","uri":"/en/applying-1password-open-source-plan/"},{"categories":["Guides"],"content":"Application Process Create a 1Password Teams account Go to the registration link and fill in the registration information After registration, you‚Äôll notice the account is in trial status, don‚Äôt worry Invite at least one other user to join the team‚Äôs Owners group When applying, please fill out this form and submit a new issue in this repository. ","date":"2025-03-14","objectID":"/en/applying-1password-open-source-plan/:3:0","tags":["Apply","1Password","OpenSource"],"title":"Apply for 1Password OpenSource Plan Through Open Source Projects to Get Teams Subscription License","uri":"/en/applying-1password-open-source-plan/"},{"categories":["Guides"],"content":"After Application Completion Go to Team Management \u003e Billing, and you will no longer see any words related to expiration. The owner of 1Password Team cannot see other users‚Äô private data, but can share items with others in the Shared vault. Other than that, there‚Äôs not much difference from 1Password Individual. However, please note: This membership cannot be transferred or sold, so the owner of this 1Password for Teams can only be you; additionally, if your open source project is no longer active, 1Password may revoke your membership. ","date":"2025-03-14","objectID":"/en/applying-1password-open-source-plan/:4:0","tags":["Apply","1Password","OpenSource"],"title":"Apply for 1Password OpenSource Plan Through Open Source Projects to Get Teams Subscription License","uri":"/en/applying-1password-open-source-plan/"},{"categories":["Guides"],"content":"What Are the Benefits of 1Password? Supports macOS, iOS, Windows, Android, Linux, browser extensions, and command line Can generate SSH Key pairs and use 1Password-CLI for Git Commit signing or other automated workflows Calling 1Password-CLI or waking up the computer can quickly authenticate through Windows Hello / Touch ID / Face ID (may support Vision Pro‚Äôs Optic ID in the future), but after computer restart or long periods without login, you must manually enter the master password Instead of using similar passwords for every website, using 1Password allows you to generate different passwords for each website, preventing credential stuffing that could expose passwords for other sites Compared to saving passwords in the browser, 1Password can reduce the risk of account credentials being stolen when your computer is infiltrated by malware Self-hosting Bitwarden is a good low-cost solution, but it has many instability factors. Once the server goes down, data becomes inaccessible. I personally believe the 1Password team has better ops skills than me. üôÉ The rest you may need to explore yourself‚Ä¶ ","date":"2025-03-14","objectID":"/en/applying-1password-open-source-plan/:5:0","tags":["Apply","1Password","OpenSource"],"title":"Apply for 1Password OpenSource Plan Through Open Source Projects to Get Teams Subscription License","uri":"/en/applying-1password-open-source-plan/"},{"categories":["Build"],"content":"Install Environment Dependencies brew install automake autoconf libtool pkg-config texinfo coreutils gnu-getopt \\ python@3 cmake ninja ccache bison byacc gettext wget pcre maven llvm@16 openjdk@17 npmDoris master currently only supports JDK 17 Environment variables that need to be set: export JAVA_HOME=\"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\" export PATH=$JAVA_HOME/bin:$PATH export PATH=\"/opt/homebrew/opt/openjdk@17/bin:$PATH\" export PATH=\"/opt/homebrew/opt/texinfo/bin:$PATH\"","date":"2025-03-09","objectID":"/en/build-doris-on-macbook-m1/:1:0","tags":["Doris","MacBook","M1"],"title":"Build Doris on MacBook M1","uri":"/en/build-doris-on-macbook-m1/"},{"categories":["Build"],"content":"Clone Your Code Clone the repository cd ~ mkdir DorisDev cd DorisDev git clone https://github.com/GitHubID/doris.git Set environment variables export DORIS_HOME=~/DorisDev/doris export PATH=$DORIS_HOME/bin:$PATH ","date":"2025-03-09","objectID":"/en/build-doris-on-macbook-m1/:2:0","tags":["Doris","MacBook","M1"],"title":"Build Doris on MacBook M1","uri":"/en/build-doris-on-macbook-m1/"},{"categories":["Build"],"content":"Download Doris Build Dependencies The Apache Doris Third Party Prebuilt page has all third-party library sources, you can directly download doris-thirdparty-source.tgz. You can directly download pre-built third-party libraries from the Apache Doris Third Party Prebuilt page, saving the process of compiling third-party libraries. Refer to the commands below. cd thirdparty rm -rf installed # Intel chip curl -L https://github.com/apache/doris-thirdparty/releases/download/automation/doris-thirdparty-prebuilt-darwin-x86_64.tar.xz \\ -o - | tar -Jxf - # Apple Silicon chip curl -L https://github.com/apache/doris-thirdparty/releases/download/automation/doris-thirdparty-prebuilt-darwin-arm64.tar.xz \\ -o - | tar -Jxf - # Ensure protoc and thrift can run properly cd installed/bin ./protoc --version ./thrift --version When running protoc and thrift, you may encounter the ‚Äúcannot be opened because the developer cannot be verified‚Äù issue. You can go to Security \u0026 Privacy. Click the Open Anyway button in the General panel to confirm you want to open the binary. Refer to https://support.apple.com/HT202491. ","date":"2025-03-09","objectID":"/en/build-doris-on-macbook-m1/:3:0","tags":["Doris","MacBook","M1"],"title":"Build Doris on MacBook M1","uri":"/en/build-doris-on-macbook-m1/"},{"categories":["Build"],"content":"Modify System Maximum File Handle Limit # bash echo 'ulimit -n 65536' \u003e\u003e~/.bashrc # zsh echo 'ulimit -n 65536' \u003e\u003e~/.zshrc","date":"2025-03-09","objectID":"/en/build-doris-on-macbook-m1/:4:0","tags":["Doris","MacBook","M1"],"title":"Build Doris on MacBook M1","uri":"/en/build-doris-on-macbook-m1/"},{"categories":["Build"],"content":"Build Doris cd $DORIS_HOME sh build.sh","date":"2025-03-09","objectID":"/en/build-doris-on-macbook-m1/:5:0","tags":["Doris","MacBook","M1"],"title":"Build Doris on MacBook M1","uri":"/en/build-doris-on-macbook-m1/"},{"categories":["Build"],"content":"You May Encounter Errors Caused by High Version Node.js During Build Haven‚Äôt encountered this on M1. opensslErrorStack: [‚Äôerror:03000086:digital envelope routines::initialization error‚Äô] library: ‚Äòdigital envelope routines‚Äô reason: ‚Äòunsupported‚Äô code: ‚ÄòERR_OSSL_EVP_UNSUPPORTED‚Äô The following command solves the problem. Refer to https://stackoverflow.com/questions/74726224/opensslerrorstack-error03000086digital-envelope-routinesinitialization-e # Tell Node.js to use legacy OpenSSL provider export NODE_OPTIONS=--openssl-legacy-provider","date":"2025-03-09","objectID":"/en/build-doris-on-macbook-m1/:6:0","tags":["Doris","MacBook","M1"],"title":"Build Doris on MacBook M1","uri":"/en/build-doris-on-macbook-m1/"},{"categories":["Build"],"content":"You May Encounter jni.h Not Found During Build Mac‚Äôs JDK is in a deeper directory. export JAVA_HOME=\"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\" export PATH=$JAVA_HOME/bin:$PATH","date":"2025-03-09","objectID":"/en/build-doris-on-macbook-m1/:7:0","tags":["Doris","MacBook","M1"],"title":"Build Doris on MacBook M1","uri":"/en/build-doris-on-macbook-m1/"},{"categories":["Build"],"content":"Configure Debug Environment # Copy the compiled package cp -r output ../doris-run # Configure FE/BE conf, default settings are fine 1. IP, directory 2. BE additional configuration: min_file_descriptor_number = 10000","date":"2025-03-09","objectID":"/en/build-doris-on-macbook-m1/:8:0","tags":["Doris","MacBook","M1"],"title":"Build Doris on MacBook M1","uri":"/en/build-doris-on-macbook-m1/"},{"categories":["Build"],"content":"Start Debugging with IDE Refer to the official documentation: CLion Mac Debug BE IntelliJ IDEA Mac Debug FE ","date":"2025-03-09","objectID":"/en/build-doris-on-macbook-m1/:9:0","tags":["Doris","MacBook","M1"],"title":"Build Doris on MacBook M1","uri":"/en/build-doris-on-macbook-m1/"},{"categories":null,"content":"About LoveIt","date":"2019-08-02","objectID":"/en/about/","tags":null,"title":"About LoveIt","uri":"/en/about/"},{"categories":null,"content":" ¬†LoveIt is a clean, elegant but advanced blog theme for Hugo developed by ‚ÄâDillon. It is based on the original LeaveIt Theme and KeepIt Theme. Hugo Theme LoveIt ","date":"2019-08-02","objectID":"/en/about/:0:0","tags":null,"title":"About LoveIt","uri":"/en/about/"},{"categories":null,"content":"Features ","date":"2019-08-02","objectID":"/en/about/:1:0","tags":null,"title":"About LoveIt","uri":"/en/about/"},{"categories":null,"content":"Performance and SEO ¬†Optimized for performance: 99/100 on mobile and 100/100 on desktop in Google PageSpeed Insights ¬†Optimized SEO performance with a correct SEO SCHEMA based on JSON-LD ¬†Google Analytics supported ¬†Fathom Analytics supported ¬†Plausible Analytics supported ¬†Yandex Metrica supported ¬†Search engine verification supported (Google, Bind, Yandex and Baidu) ¬†CDN for third-party libraries supported ¬†Automatically converted images with Lazy Load by lazysizes ","date":"2019-08-02","objectID":"/en/about/:1:1","tags":null,"title":"About LoveIt","uri":"/en/about/"},{"categories":null,"content":"Appearance and Layout ¬†Desktop/Mobile responsive layout ¬†Light/Dark mode ¬†Globally consistent design language ¬†Pagination supported ¬†Easy-to-use and self-expanding table of contents ¬†Multilanguage supported and i18n ready ¬†Beautiful CSS animation ","date":"2019-08-02","objectID":"/en/about/:1:2","tags":null,"title":"About LoveIt","uri":"/en/about/"},{"categories":null,"content":"Social and Comment Systems ¬†Gravatar supported by Gravatar ¬†Local Avatar supported ¬†Up to 76 social links supported ¬†Up to 24 share sites supported ¬†Disqus comment system supported by Disqus ¬†Gitalk comment system supported by Gitalk ¬†Valine comment system supported by Valine ¬†Facebook comments system supported by Facebook ¬†Telegram comments system supported by Comments ¬†Commento comment system supported by Commento ¬†utterances comment system supported by utterances ¬†giscus comment system supported by giscus ","date":"2019-08-02","objectID":"/en/about/:1:3","tags":null,"title":"About LoveIt","uri":"/en/about/"},{"categories":null,"content":"Extended Features ¬†Search supported by Lunr.js or algolia ¬†Twemoji supported ¬†Automatically highlighting code ¬†Copy code to clipboard with one click ¬†Images gallery supported by lightGallery ¬†Extended Markdown syntax for Font Awesome icons ¬†Extended Markdown syntax for ruby annotation ¬†Extended Markdown syntax for fraction ¬†Mathematical formula supported by $\\KaTeX$ ¬†Diagrams shortcode supported by mermaid ¬†Interactive data visualization shortcode supported by ECharts ¬†Mapbox shortcode supported by Mapbox GL JS ¬†Music player shortcode supported by APlayer and MetingJS ¬†Bilibili player shortcode ¬†Kinds of admonitions shortcode ¬†Custom style shortcode ¬†Custom script shortcode ¬†Animated typing supported by TypeIt ¬†Cookie consent banner supported by cookieconsent ¬†Person shortcode ‚Ä¶ ","date":"2019-08-02","objectID":"/en/about/:1:4","tags":null,"title":"About LoveIt","uri":"/en/about/"},{"categories":null,"content":"License LoveIt is licensed under the MIT license. Check the LICENSE file for details. ","date":"2019-08-02","objectID":"/en/about/:2:0","tags":null,"title":"About LoveIt","uri":"/en/about/"},{"categories":null,"content":"Special Thanks Thanks to the authors of following resources included in the theme: modern-normalize Font Awesome Simple Icons Animate.css autocomplete Lunr.js algoliasearch lazysizes object-fit-images Twemoji emoji-data lightGallery clipboard.js Sharer.js TypeIt $\\KaTeX$ mermaid ECharts Mapbox GL JS APlayer MetingJS Gitalk Valine cookieconsent ","date":"2019-08-02","objectID":"/en/about/:3:0","tags":null,"title":"About LoveIt","uri":"/en/about/"}]