<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning - LoveIt</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel=stylesheet><meta name=Description content="Hugo theme - LoveIt"><meta property="og:url" content="https://koschei.top/en/cs336-assign5/"><meta property="og:site_name" content="LoveIt"><meta property="og:title" content="[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning"><meta property="og:description" content="1 Assignment Overview In this assignment, you will gain hands-on experience in training language models to reason when solving math problems.
What to Implement Implement a zero-shot prompting baseline for the MATH competition dataset proposed by Hendrycks et al. [2021]. Supervised Fine-Tuning (SFT) using reasoning traces from a stronger reasoning model (DeepSeek R1, DeepSeekAI et al., 2025). Use Expert Iteration to improve reasoning performance through verification rewards. Use Group Relative Policy Optimization (GRPO) to improve reasoning performance through verification rewards. For interested students, we will release an optional part of the assignment in the coming days: aligning language models with human preferences."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-13T18:12:00+08:00"><meta property="article:modified_time" content="2026-01-31T01:43:03+08:00"><meta property="article:tag" content="CS336"><meta property="article:tag" content="LLM"><meta property="og:image" content="https://koschei.top/logo.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://koschei.top/logo.png"><meta name=twitter:title content="[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning"><meta name=twitter:description content="1 Assignment Overview In this assignment, you will gain hands-on experience in training language models to reason when solving math problems.
What to Implement Implement a zero-shot prompting baseline for the MATH competition dataset proposed by Hendrycks et al. [2021]. Supervised Fine-Tuning (SFT) using reasoning traces from a stronger reasoning model (DeepSeek R1, DeepSeekAI et al., 2025). Use Expert Iteration to improve reasoning performance through verification rewards. Use Group Relative Policy Optimization (GRPO) to improve reasoning performance through verification rewards. For interested students, we will release an optional part of the assignment in the coming days: aligning language models with human preferences."><meta name=twitter:site content="@xxxx"><meta name=application-name content="LoveIt"><meta name=apple-mobile-web-app-title content="LoveIt"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://koschei.top/en/cs336-assign5/><link rel=prev href=https://koschei.top/en/cs336-assign1/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/koschei.top\/en\/cs336-assign5\/"},"image":[{"@type":"ImageObject","url":"https:\/\/koschei.top\/images\/Apple-Devices-Preview.png","width":3200,"height":2048}],"genre":"posts","keywords":"CS336, LLM","wordcount":1997,"url":"https:\/\/koschei.top\/en\/cs336-assign5\/","datePublished":"2026-01-13T18:12:00+08:00","dateModified":"2026-01-31T01:43:03+08:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"xxxx","logo":{"@type":"ImageObject","url":"https:\/\/koschei.top\/images\/avatar.png","width":960,"height":960}},"author":{"@type":"Person","name":"Koschei"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><canvas id=particles-canvas></canvas><div id=reading-progress-bar></div><style>#particles-canvas{position:fixed;top:0;left:0;width:100%;height:100%;pointer-events:none;z-index:-1;opacity:.6}[theme=dark] #particles-canvas{opacity:.4}#reading-progress-bar{position:fixed;top:0;left:0;width:0%;height:3px;background:linear-gradient(90deg,#6366f1,#8b5cf6,#a855f7,#ec4899);z-index:9999;transition:width 50ms ease-out;box-shadow:0 0 10px rgba(139,92,246,.5)}</style><script>(function(){const e=document.getElementById("particles-canvas"),t=e.getContext("2d");let n=[],i;const l=80,a=["#6366f1","#8b5cf6","#a855f7","#ec4899","#3b82f6","#14b8a6"];function r(){e.width=window.innerWidth,e.height=window.innerHeight}class d{constructor(){this.reset()}reset(){const n=e.width/2,s=e.height/2;this.angle=Math.random()*Math.PI*2;const t=Math.random()*100;this.x=n+Math.cos(this.angle)*t,this.y=s+Math.sin(this.angle)*t,this.speed=.2+Math.random()*.5,this.vx=Math.cos(this.angle)*this.speed,this.vy=Math.sin(this.angle)*this.speed,this.length=8+Math.random()*15,this.width=2+Math.random()*2,this.color=a[Math.floor(Math.random()*a.length)],this.opacity=.3+Math.random()*.5,this.rotation=this.angle,this.pulseSpeed=.01+Math.random()*.02,this.pulseOffset=Math.random()*Math.PI*2}update(){this.x+=this.vx,this.y+=this.vy,this.opacity=.3+Math.sin(Date.now()*this.pulseSpeed+this.pulseOffset)*.2;const t=100;(this.x<-t||this.x>e.width+t||this.y<-t||this.y>e.height+t)&&this.reset()}draw(){t.save(),t.translate(this.x,this.y),t.rotate(this.rotation),t.globalAlpha=this.opacity,t.beginPath(),t.roundRect(-this.length/2,-this.width/2,this.length,this.width,this.width/2),t.fillStyle=this.color,t.fill(),t.restore()}}function c(){n=[];for(let s=0;s<l;s++){const t=new d,o=Math.random()*Math.max(e.width,e.height)/2;t.x=e.width/2+Math.cos(t.angle)*o,t.y=e.height/2+Math.sin(t.angle)*o,n.push(t)}}function s(){t.clearRect(0,0,e.width,e.height);const o=t.createRadialGradient(e.width/2,e.height/2,0,e.width/2,e.height/2,Math.max(e.width,e.height)/2);o.addColorStop(0,"rgba(139, 92, 246, 0.03)"),o.addColorStop(.5,"rgba(99, 102, 241, 0.01)"),o.addColorStop(1,"transparent"),t.fillStyle=o,t.fillRect(0,0,e.width,e.height),n.forEach(e=>{e.update(),e.draw()}),i=requestAnimationFrame(s)}r(),c(),s(),window.addEventListener("resize",()=>{r(),c()}),document.addEventListener("visibilitychange",()=>{document.hidden?cancelAnimationFrame(i):s()});const u=document.getElementById("reading-progress-bar");function o(){const t=window.scrollY,e=document.documentElement.scrollHeight-window.innerHeight,n=e>0?t/e*100:0;u.style.width=n+"%"}window.addEventListener("scroll",o,{passive:!0}),window.addEventListener("resize",o,{passive:!0}),o()})()</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/en/ title=LoveIt><span class=header-title-pre><i class='far fa-kiss-wink-heart fa-fw' aria-hidden=true></i></span>LoveIt</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/en/posts/>Posts </a><a class=menu-item href=/en/tags/>Tags </a><a class=menu-item href=/en/categories/>Categories </a><a class=menu-item href=/en/categories/documentation/>Docs </a><a class=menu-item href=/en/about/>About </a><a class=menu-item href=https://github.com/dillonzq/LoveIt title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i> </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="menu-item language" title="Select Language"><i class="fa fa-globe fa-fw" aria-hidden=true></i>
<select class=language-select id=language-select-desktop onchange="location=this.value"><option value=/en/cs336-assign5/ selected>English</option><option value=/cs336-assign5/>简体中文</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/en/ title=LoveIt><span class=header-title-pre><i class='far fa-kiss-wink-heart fa-fw' aria-hidden=true></i></span>LoveIt</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/en/posts/ title>Posts</a><a class=menu-item href=/en/tags/ title>Tags</a><a class=menu-item href=/en/categories/ title>Categories</a><a class=menu-item href=/en/categories/documentation/ title>Docs</a><a class=menu-item href=/en/about/ title>About</a><a class=menu-item href=https://github.com/dillonzq/LoveIt title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class=menu-item title="Select Language"><i class="fa fa-globe fa-fw" aria-hidden=true></i>
<select class=language-select onchange="location=this.value"><option value=/en/cs336-assign5/ selected>English</option><option value=/cs336-assign5/>简体中文</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/kosthi title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Koschei</a></span>&nbsp;<span class=post-category>included in <a href=/en/categories/llm/><i class="far fa-folder fa-fw" aria-hidden=true></i>LLM</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2026-01-13>2026-01-13</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;1997 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;10 minutes&nbsp;<span id=busuanzi_container_page_pv>
<i class="far fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_page_pv></span>&nbsp;views
</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#1-assignment-overview>1 Assignment Overview</a><ul><li><a href=#what-to-implement>What to Implement</a></li><li><a href=#what-to-run>What to Run</a></li></ul></li><li><a href=#2-reasoning-capabilities-of-language-models>2 Reasoning Capabilities of Language Models</a><ul><li><a href=#21-motivation>2.1 Motivation</a></li><li><a href=#22-chain-of-thought-reasoning-and-reasoning-reinforcement-learning>2.2 Chain-of-Thought Reasoning and Reasoning Reinforcement Learning</a><ul><li><a href=#chain-of-thought-reasoning-in-language-models>Chain-of-Thought Reasoning in Language Models</a></li><li><a href=#learning-reasoning-via-expert-iteration>Learning Reasoning via Expert Iteration</a></li><li><a href=#reasoning-reinforcement-learning-with-verification-rewards-o1-r1>Reasoning Reinforcement Learning with Verification Rewards (o1, R1)</a></li><li><a href=#experimental-setup-models-and-datasets>Experimental Setup: Models and Datasets</a></li><li><a href=#open-source-alternative-datasets-for-open-source-auditors>Open Source Alternative Datasets (For Open Source Auditors)</a></li></ul></li></ul></li><li><a href=#3-evaluating-zero-shot-math-performance>3 Evaluating Zero-Shot MATH Performance</a><ul><li><a href=#problem-math_baseline_performance-4-points>Problem (math_baseline_performance): 4 points</a><ul><li><a href=#a-write-a-script-to-evaluate-qwen-25-math-15b-models-zero-shot-performance-on-the-math-dataset>(a) Write a script to evaluate Qwen 2.5 Math 1.5B model&rsquo;s zero-shot performance on the MATH dataset</a></li><li><a href=#b-run-your-evaluation-script-on-the-qwen-25-math-15b-model>(b) Run your evaluation script on the Qwen 2.5 Math 1.5B model</a></li><li><a href=#c-how-does-the-qwen-25-math-15b-model-perform-on-zero-shot-baseline-on-the-math-dataset>(c) How does the Qwen 2.5 Math 1.5B model perform on zero-shot baseline on the MATH dataset?</a></li></ul></li></ul></li><li><a href=#4-supervised-fine-tuning-for-math>4 Supervised Fine-Tuning for MATH</a><ul><li><a href=#problem-sft_experiment-run-sft-on-math-dataset-2-points-2-h100-hours>Problem (sft_experiment): Run SFT on MATH dataset (2 points) (2 H100 hours)</a><ul><li><ul><li><a href=#1-using-qwen-25-math-15b-base-model-run-sft-on-reasoning-sft-examples-provided-in-dataa5-alignmentmathsftjsonl-varying-the-number-of-unique-examples-in-sft-in-128-256-512-1024-and-using-the-full-dataset-tune-learning-rate-and-batch-size-to-achieve-at-least-15-validation-accuracy-when-using-the-full-dataset>1. Using Qwen 2.5 Math 1.5B base model, run SFT on reasoning SFT examples (provided in /data/a5-alignment/MATH/sft.jsonl), varying the number of unique examples in SFT in {128, 256, 512, 1024}, and using the full dataset. Tune learning rate and batch size to achieve at least 15% validation accuracy when using the full dataset.</a></li><li><a href=#2-filter-reasoning-sft-examples-to-include-only-those-producing-correct-answers-run-sft-on-the-full-filtered-dataset-and-report-the-filtered-dataset-size-and-validation-accuracy-achieved>2. Filter reasoning SFT examples to include only those producing correct answers. Run SFT on the (full) filtered dataset and report the filtered dataset size and validation accuracy achieved.</a></li></ul></li></ul></li></ul></li><li><a href=#5-expert-iteration-for-math>5 Expert Iteration for MATH</a><ul><li><a href=#problem-expert_iteration_experiment-run-expert-iteration-on-math-dataset-2-points-6-h100-hours>Problem (expert_iteration_experiment): Run Expert Iteration on MATH dataset (2 points) (6 H100 hours)</a><ul><li><a href=#run-expert-iteration-on-the-math-dataset-provided-in-dataa5-alignmentmathtrainjsonl-using-qwen-25-math-15b-base-model-varying-the-number-of-rollouts-per-question-g-and-number-of-epochs-used-in-sft-step-using-n_ei_steps--5-vary-batch-size-for-each-expert-iteration-step-ie-size-of-db-in-512-1024-2048-you-dont-need-to-try-all-possible-combinations-of-these-hyperparameters-just-enough-to-draw-conclusions-about-each-record-entropy-of-model-responses-during-training-make-sure-vllm-terminates-generation-at-the-second-answer-tag--as-done-in-sft-section>Run expert iteration on the MATH dataset (provided in /data/a5-alignment/MATH/train.jsonl) using Qwen 2.5 Math 1.5B Base model, varying the number of rollouts per question G and number of epochs used in SFT step, using n_ei_steps = 5. Vary batch size for each expert iteration step (i.e., size of Db) in {512, 1024, 2048}. (You don’t need to try all possible combinations of these hyperparameters. Just enough to draw conclusions about each.) Record entropy of model responses during training. Make sure vLLM terminates generation at the second answer tag as done in SFT section.</a></li></ul></li></ul></li><li><a href=#6-policy-gradient-primer>6 Policy Gradient Primer</a></li></ul></nav></div></div><div class=content id=content><h2 id=1-assignment-overview>1 Assignment Overview</h2><p>In this assignment, you will gain hands-on experience in training language models to reason when solving math problems.</p><h3 id=what-to-implement>What to Implement</h3><ol><li>Implement a zero-shot prompting baseline for the MATH competition dataset proposed by Hendrycks et al. [2021].</li><li>Supervised Fine-Tuning (SFT) using reasoning traces from a stronger reasoning model (DeepSeek R1, DeepSeekAI et al., 2025).</li><li>Use Expert Iteration to improve reasoning performance through verification rewards.</li><li>Use Group Relative Policy Optimization (GRPO) to improve reasoning performance through verification rewards.</li></ol><p>For interested students, we will release an optional part of the assignment in the coming days: aligning language models with human preferences.</p><h3 id=what-to-run>What to Run</h3><ol><li>Evaluate the zero-shot prompting performance of the Qwen 2.5 Math 1.5B model (as a baseline).</li><li>Supervised fine-tuning of Qwen 2.5 Math 1.5B using R1&rsquo;s reasoning traces.</li><li>Expert iteration training of Qwen 2.5 Math 1.5B using verification rewards.</li><li>GRPO training of Qwen 2.5 Math 1.5B using verification rewards.</li></ol><p>Assignment link:
<a href=https://github.com/Kosthi/assignment5-alignment target=_blank rel="noopener noreffer">Assignment5-alignment GitHub Repository</a></p><p>Next, I will share some details and insights from completing the assignment.</p><h2 id=2-reasoning-capabilities-of-language-models>2 Reasoning Capabilities of Language Models</h2><h3 id=21-motivation>2.1 Motivation</h3><p>One prominent application of language models is building general-purpose systems that can handle various natural language processing tasks. This assignment focuses on an emerging application: mathematical reasoning. We will use this as a testbed to build evaluation systems, perform supervised fine-tuning, and explore methods for training language models to reason using reinforcement learning (RL).</p><p>This assignment differs from previous ones in two ways:</p><ol><li>We no longer use the language model codebase and models from previous assignments. Ideally, we would use the base language models trained in previous assignments, but fine-tuning those models won&rsquo;t yield satisfactory results—they are too weak to demonstrate complex mathematical reasoning. Therefore, we switch to an accessible, modern, high-performance language model (Qwen 2.5 Math 1.5B Base) for most of our work.</li><li>We introduce a new benchmark dataset to evaluate language models. Previously, we considered cross-entropy as a good proxy for many downstream tasks. However, the core of this assignment is narrowing the gap between base models and downstream tasks, so we must use evaluation methods independent of cross-entropy. We will use the MATH 12K dataset proposed by Hendrycks et al. [2021], which contains challenging high school competition math problems. We evaluate language model performance by comparing model outputs against reference answers.</li></ol><h3 id=22-chain-of-thought-reasoning-and-reasoning-reinforcement-learning>2.2 Chain-of-Thought Reasoning and Reasoning Reinforcement Learning</h3><p>A recent hot trend in language models is using Chain-of-Thought (CoT) reasoning to improve performance on various tasks. Chain-of-thought refers to the process of reasoning through a problem step by step, generating intermediate reasoning steps before arriving at the final answer.</p><h4 id=chain-of-thought-reasoning-in-language-models>Chain-of-Thought Reasoning in Language Models</h4><p>Early chain-of-thought methods decomposed problems into intermediate steps using a &ldquo;scratchpad&rdquo; and fine-tuned language models to solve simple math tasks like arithmetic [Nye et al., 2021]. Other research prompted strong models to &ldquo;think step by step&rdquo; before answering, finding this significantly improved performance on math reasoning tasks like elementary arithmetic [Wei et al., 2023].</p><h4 id=learning-reasoning-via-expert-iteration>Learning Reasoning via Expert Iteration</h4><p>Self-Taught Reasoner (STaR) [Zelikman et al., 2022] constructs reasoning as a bootstrapping loop: the pretrained model first generates diverse chains of thought (CoTs), retaining only those that produce correct answers, then fine-tuning on these &ldquo;expert&rdquo; trajectories. Repeating this cycle improves the language model&rsquo;s reasoning ability and problem-solving rate. STaR demonstrated that this expert iteration approach [Anthony et al., 2017] can bootstrap reasoning capabilities through automatic string matching verification without human-written reasoning trajectories.</p><h4 id=reasoning-reinforcement-learning-with-verification-rewards-o1-r1>Reasoning Reinforcement Learning with Verification Rewards (o1, R1)</h4><p>Recent research explores using more powerful reinforcement learning algorithms combined with verification rewards to improve reasoning performance. OpenAI&rsquo;s o1 (and subsequent o3/o4) [OpenAI et al., 2024], DeepSeek&rsquo;s R1 [DeepSeek-AI et al., 2025], and Moonshot&rsquo;s kimi k1.5 [Team et al., 2025] all use policy gradient methods [Sutton et al., 1999] to train on math and code tasks, verifying answer correctness through string matching or unit tests, achieving significant performance improvements on competition math and programming tasks. Follow-up work like Open-R1 [Face, 2025], SimpleRL-Zoo [Zeng et al., 2025], and TinyZero [Pan et al., 2025] demonstrate that pure reinforcement learning with verification rewards can improve reasoning performance even on models with only 1.5B parameters.</p><h4 id=experimental-setup-models-and-datasets>Experimental Setup: Models and Datasets</h4><p>In the following sections, we will progressively adopt more complex methods to train base language models to solve math problems through step-by-step reasoning. This assignment uses the Qwen 2.5 Math 1.5B Base model, which is based on the Qwen 2.5 1.5B model and underwent continued pretraining on high-quality synthetic math pretraining data [Yang et al., 2024]. The MATH dataset is available at <code>/data/a5-alignment/MATH</code> on the Together cluster.</p><h4 id=open-source-alternative-datasets-for-open-source-auditors>Open Source Alternative Datasets (For Open Source Auditors)</h4><p>Due to copyright restrictions, the MATH dataset is not publicly available. If you&rsquo;re completing the assignment locally, you can use the following open-source math reasoning datasets:</p><ul><li>Countdown [Pan et al., 2025]: A simple synthetic task based on the UK TV show &ldquo;Countdown&rdquo;, commonly used as a testbed for small-scale reasoning RL.</li><li>GSM8K [Cobbe et al., 2021a]: Elementary arithmetic problem dataset, easier than MATH, suitable for debugging code correctness and familiarizing with reasoning RL workflow.</li><li>Tulu 3 SFT Math [Lambert et al., 2025]: Synthetic math problems generated using GPT-4o and Claude 3.5 Sonnet. Since it&rsquo;s synthetic data, some answers (or even questions) may be incorrect.</li><li>Other math SFT datasets available online.</li></ul><p>If the dataset doesn&rsquo;t directly provide short ground truth labels (like 1/2), you can use math answer parsers like Math-Verify to process the ground truth label column.</p><h2 id=3-evaluating-zero-shot-math-performance>3 Evaluating Zero-Shot MATH Performance</h2><p>We first evaluate the base language model&rsquo;s performance on the MATH dataset&rsquo;s 5K test set. Establishing this baseline helps understand how subsequent methods affect model behavior.</p><h3 id=problem-math_baseline_performance-4-points>Problem (math_baseline_performance): 4 points</h3><h4 id=a-write-a-script-to-evaluate-qwen-25-math-15b-models-zero-shot-performance-on-the-math-dataset>(a) Write a script to evaluate Qwen 2.5 Math 1.5B model&rsquo;s zero-shot performance on the MATH dataset</h4><p>The script should complete the following tasks:</p><ol><li>Load MATH validation samples from <code>/data/a5-alignment/MATH/validation.jsonl</code>;</li><li>Format these samples into string prompts acceptable by language models using the <code>r1_zero</code> prompt;</li><li>Generate model outputs for each sample;</li><li>Calculate evaluation metrics;</li><li>Serialize samples, model generations, and corresponding evaluation scores to disk for subsequent analysis.</li></ol><p>During implementation, writing an <code>evaluate_vllm</code> method like below will be helpful, which you can reuse later:</p><div class="code-block code-line-numbers open" style="counter-reset:code-block 0"><div class="code-header language-bash"><span class=code-title><i class="arrow fas fa-chevron-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title="Copy to clipboard"><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>evaluate_vllm</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>vllm_model</span><span class=p>:</span> <span class=n>LLM</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>reward_fn</span><span class=p>:</span> <span class=n>Callable</span><span class=p>[[</span><span class=nb>str</span><span class=p>,</span> <span class=nb>str</span><span class=p>],</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=nb>float</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>    <span class=n>prompts</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>str</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>eval_sampling_params</span><span class=p>:</span> <span class=n>SamplingParams</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Evaluate language model performance on a set of prompts,
</span></span></span><span class=line><span class=cl><span class=s2>    calculate evaluation metrics, and serialize results to disk.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span></span></span></code></pre></div></div><p><strong>Submission</strong>: A script for evaluating zero-shot MATH dataset baseline performance.</p><p>See at <a href=https://github.com/Kosthi/assignment5-alignment/blob/main/eval.py target=_blank rel="noopener noreffer">eval.py</a></p><h4 id=b-run-your-evaluation-script-on-the-qwen-25-math-15b-model>(b) Run your evaluation script on the Qwen 2.5 Math 1.5B model</h4><p>Count the number of model generations falling into each of the following three categories:</p><ol><li>Both format reward and answer reward are 1 (completely correct);</li><li>Format reward is 1, answer reward is 0 (format correct, answer wrong);</li><li>Both format reward and answer reward are 0 (both format and answer wrong).</li></ol><p>Observe at least 10 cases with format reward 0. Do you think the problem lies with the base model&rsquo;s output or the parser? Why? Then observe at least 10 cases with format reward 1 but answer reward 0. What are your thoughts?</p><p><strong>Submission</strong>: Analysis of model and reward function performance, including examples of each category.</p><p><strong>1. Count Statistics</strong>
Based on the summary at the beginning of the log (<code>num_examples=5000</code>) and category counts, the model generation distribution is:</p><ul><li><strong>Both format reward and answer reward are 1 (completely correct)</strong>: <code>count(fmt=1,ans=1)=139</code> cases.</li><li><strong>Format reward is 1, answer reward is 0 (format correct, answer wrong)</strong>: <code>count(fmt=1,ans=0)=693</code> cases.</li><li><strong>Both format reward and answer reward are 0 (both wrong)</strong>: <code>count(fmt=0,ans=0)=4168</code> cases.</li></ul><p><strong>2. Observations on &ldquo;Format Reward 0&rdquo; Cases</strong>
In at least 10 such cases from the log (marked as <code>sample_fmt0_ans0_examples=10</code>), <strong>the problem mainly lies with the base model&rsquo;s output</strong>, not the parser.</p><p><strong>Main Reason</strong>: The common issue in these failure cases is that the model did not follow the specified output format. According to the log, the correct format should include clear <code>&lt;think></code> reasoning and a final answer wrapped in <code>&lt;answer></code> tags. However, problematic model outputs often show:</p><ul><li><strong>Confused format structure</strong>: For example, when answering &ldquo;What is the positive difference between $120%$ of 30 and $130%$ of 20?&rdquo;, the model gave the answer directly within <code>&lt;think></code> tags and mixed in undefined tags like <code>&lt;end think> &lt;answer></code>, causing parsing failure.</li><li><strong>Irrelevant content and symbols</strong>: For example, when answering the inverse function problem &ldquo;Let $f(x)=7x+5$&mldr;&rdquo;, the <code>&lt;think></code> content contained lots of irrelevant code snippets, garbled characters, and even image links, completely deviating from mathematical reasoning.</li><li><strong>Reasoning and answer not separated</strong>: The model often wrote &ldquo;the answer is&mldr;&rdquo; in the thinking part, or the final answer wasn&rsquo;t wrapped in required tags.</li></ul><p>These phenomena indicate that the base model has difficulty following strict structured output instructions and fails to generate text that parsers can correctly process.</p><p><strong>3. Observations on &ldquo;Format Reward 1 but Answer Reward 0&rdquo; Cases</strong>
In at least 10 such cases from the log (marked as <code>sample_fmt1_ans0_examples=10</code>), the model successfully followed format requirements, but the answer itself was incorrect.</p><p><strong>Main Observations</strong>: This reveals the model&rsquo;s <strong>core capability limitations</strong>. With correct formatting, error types include:</p><ul><li><strong>Mathematical calculation errors</strong>: For example, calculating <code>$i^5+i^{-25}+i^{45}$</code>, the model got <code>$2i$</code>, but the correct answer should be <code>$i$</code>.</li><li><strong>Logical reasoning errors</strong>: For example, finding integers satisfying <code>$|x|+1>7$</code> and <code>$|x+1|\le7$</code>, the model correctly listed calculation steps but incorrectly included <code>-6</code>, getting sum <code>-21</code> when the correct answer is <code>-15</code>.</li><li><strong>Imprecise final answer expression</strong>: For example, asking about the number of vertical lines, the model correctly gave values causing denominator to be zero &ldquo;<code>-3$ and $2$</code>&rdquo; in <code>&lt;answer></code>, but didn&rsquo;t convert it to the final answer &ldquo;2 lines&rdquo;.</li></ul><p>This indicates that the model has somewhat learned to &ldquo;imitate&rdquo; the format and framework of problem-solving, but its mathematical reasoning, calculation accuracy, and understanding of what the problem ultimately requires remain insufficient.</p><h4 id=c-how-does-the-qwen-25-math-15b-model-perform-on-zero-shot-baseline-on-the-math-dataset>(c) How does the Qwen 2.5 Math 1.5B model perform on zero-shot baseline on the MATH dataset?</h4><p><strong>Submission</strong>: Summarize evaluation metrics in 1-2 sentences.</p><p>Based on the log metrics (<code>mean_reward=0.0278</code>, and completely correct ratio only 139/5000≈2.78%), this model has <strong>very weak zero-shot baseline performance</strong> on the MATH dataset, with extremely low overall accuracy in both format and answer, not yet capable of reliably solving complex math problems.</p><h2 id=4-supervised-fine-tuning-for-math>4 Supervised Fine-Tuning for MATH</h2><h3 id=problem-sft_experiment-run-sft-on-math-dataset-2-points-2-h100-hours>Problem (sft_experiment): Run SFT on MATH dataset (2 points) (2 H100 hours)</h3><h5 id=1-using-qwen-25-math-15b-base-model-run-sft-on-reasoning-sft-examples-provided-in-dataa5-alignmentmathsftjsonl-varying-the-number-of-unique-examples-in-sft-in-128-256-512-1024-and-using-the-full-dataset-tune-learning-rate-and-batch-size-to-achieve-at-least-15-validation-accuracy-when-using-the-full-dataset>1. Using Qwen 2.5 Math 1.5B base model, run SFT on reasoning SFT examples (provided in /data/a5-alignment/MATH/sft.jsonl), varying the number of unique examples in SFT in {128, 256, 512, 1024}, and using the full dataset. Tune learning rate and batch size to achieve at least 15% validation accuracy when using the full dataset.</h5><p><strong>Submission:</strong> Validation accuracy curves associated with different dataset sizes.</p><p>Fewer SFT samples actually work better than more samples. Entropy also drops faster and lower. With more samples, the model becomes confused instead.</p><p><img class=lazyload src=/svg/loading.min.svg data-src=./sft_experiment-1.png data-srcset="./sft_experiment-1.png, ./sft_experiment-1.png 1.5x, ./sft_experiment-1.png 2x" data-sizes=auto alt=./sft_experiment-1.png title=sft_experiment-1></p><h5 id=2-filter-reasoning-sft-examples-to-include-only-those-producing-correct-answers-run-sft-on-the-full-filtered-dataset-and-report-the-filtered-dataset-size-and-validation-accuracy-achieved>2. Filter reasoning SFT examples to include only those producing correct answers. Run SFT on the (full) filtered dataset and report the filtered dataset size and validation accuracy achieved.</h5><p><strong>Submission:</strong> Report dataset size and validation accuracy curve. Compare your findings with previous SFT experiments.</p><p>Experiment pending</p><h2 id=5-expert-iteration-for-math>5 Expert Iteration for MATH</h2><h3 id=problem-expert_iteration_experiment-run-expert-iteration-on-math-dataset-2-points-6-h100-hours>Problem (expert_iteration_experiment): Run Expert Iteration on MATH dataset (2 points) (6 H100 hours)</h3><h4 id=run-expert-iteration-on-the-math-dataset-provided-in-dataa5-alignmentmathtrainjsonl-using-qwen-25-math-15b-base-model-varying-the-number-of-rollouts-per-question-g-and-number-of-epochs-used-in-sft-step-using-n_ei_steps--5-vary-batch-size-for-each-expert-iteration-step-ie-size-of-db-in-512-1024-2048-you-dont-need-to-try-all-possible-combinations-of-these-hyperparameters-just-enough-to-draw-conclusions-about-each-record-entropy-of-model-responses-during-training-make-sure-vllm-terminates-generation-at-the-second-answer-tag--as-done-in-sft-section>Run expert iteration on the MATH dataset (provided in /data/a5-alignment/MATH/train.jsonl) using Qwen 2.5 Math 1.5B Base model, varying the number of rollouts per question G and number of epochs used in SFT step, using n_ei_steps = 5. Vary batch size for each expert iteration step (i.e., size of Db) in {512, 1024, 2048}. (You don&rsquo;t need to try all possible combinations of these hyperparameters. Just enough to draw conclusions about each.) Record entropy of model responses during training. Make sure vLLM terminates generation at the second answer tag </answer>as done in SFT section.</h4><p>Submissions:</p><ul><li>Validation accuracy curves associated with different rollout configurations. Try at least 2 different rollout counts and epoch counts.</li><li>A model achieving at least 15% validation accuracy on MATH.</li><li>A short 2-sentence discussion comparing your performance with SFT performance, as well as performance across EI steps.</li><li>A plot showing entropy of model responses during training.</li></ul><p>Experiment pending</p><h2 id=6-policy-gradient-primer>6 Policy Gradient Primer</h2></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2026-01-31&nbsp;<a class=git-hash href=https://github.com/dillonzq/LoveIt/commit/d75bc3722331736783fbd758dc16dbdfce761082 target=_blank title="commit by Koschei(nitianzero@gmail.com) d75bc3722331736783fbd758dc16dbdfce761082: feat: update cs336-assign2">
<i class="fas fa-hashtag fa-fw" aria-hidden=true></i>d75bc37</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-markdown href=/en/cs336-assign5/index.md target=_blank>Read Markdown</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on X" data-sharer=x data-url=https://koschei.top/en/cs336-assign5/ data-title="[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning" data-via=xxxx data-hashtags=CS336,LLM><i class="fab fa-x-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://koschei.top/en/cs336-assign5/ data-hashtag=CS336><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://koschei.top/en/cs336-assign5/ data-title="[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://koschei.top/en/cs336-assign5/ data-title="[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@14.9.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://koschei.top/en/cs336-assign5/ data-title="[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/en/tags/cs336/>CS336</a>,&nbsp;<a href=/en/tags/llm/>LLM</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/en/>Home</a></span></section></div><div class=post-nav><a href=/en/cs336-assign1/ class=prev rel=prev title="[Stanford CS336] Assignment 1: Building a Transformer Language Model"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>[Stanford CS336] Assignment 1: Building a Transformer Language Model</a></div></div><div id=comments><div id=twikoo class=comment></div><script src=https://cdn.jsdelivr.net/npm/twikoo@1.6.44/dist/twikoo.all.min.js></script><script>twikoo.init({envId:"https://koschei.netlify.app/.netlify/functions/twikoo",el:"#twikoo",path:location.pathname,lang:"zh-CN"});const updateTwikooTheme=()=>{const e=document.body.getAttribute("theme")==="dark";document.documentElement.setAttribute("data-theme",e?"dark":"light")};updateTwikooTheme();const observer=new MutationObserver(e=>{e.forEach(e=>{e.attributeName==="theme"&&updateTwikooTheme()})});observer.observe(document.body,{attributes:!0})</script><noscript>请启用 JavaScript 以查看由 <a href=https://twikoo.js.org/ rel="nofollow noopener noreferrer" target=_blank>Twikoo</a> 提供的评论。</noscript></div></article></div></main><footer class=footer><div class=footer-container><div id=run-time data-start-time=2025-03-04T20:39:00+08:00 data-i18n-runtime="Running for <i class='far fa-clock fa-fw'></i> %%DAYS%% days %%HOURS%% hours %%MINUTES%% minutes %%SECONDS%% seconds"></div><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.155.1">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.3.0"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2025 - 2026</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://github.com/kosthi target=_blank>Koschei</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span><span class=icp-splitter>&nbsp;|&nbsp;</span><br class=icp-br><span class=icp><a href="https://beian.mps.gov.cn/#/query/webSearch?code=33032602100584" rel=noreferrer target=_blank><img src=/备案图标.png alt=备案图标 style=vertical-align:middle;width:16px>浙公网安备33032602100584号</a></span><span class=icp-splitter>&nbsp;|&nbsp;</span><br class=icp-br><span class=icp><a href=https://beian.miit.gov.cn/ rel=noreferrer target=_blank>浙ICP备2024075396号-2</a></span></div><div class=busuanzi-footer><span id=busuanzi_container_site_pv><i class="fa fa-eye"></i> <span id=busuanzi_value_site_pv></span>
</span>&nbsp;|&nbsp;
<span id=busuanzi_container_site_uv><i class="fa fa-user"></i> <span id=busuanzi_value_site_uv></span></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i></a></div><div id=fixed-buttons-hidden><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css><script src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@5.20.2/dist/lite/builds/browser.umd.min.js></script><script src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js></script><script src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js></script><script src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js></script><script src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.2/sharer.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/copy-tex.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/mhchem.min.js></script><script>window.config={lightgallery:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{algoliaAppID:"4D1IDY8JU6",algoliaIndex:"index",algoliaSearchKey:"05332ac5ed76655a511f0da583a9afac",highlightTag:"em",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"algolia"}}</script><script src=/js/theme.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/jquery@2.1.3/dist/jquery.min.js></script><script type=text/javascript src=/js/custom.js></script></body></html>