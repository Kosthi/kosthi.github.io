<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>[Stanford CS336] Assignment 1: Building a Transformer Language Model - LoveIt</title><meta name=Description content="Hugo theme - LoveIt"><meta property="og:url" content="https://koschei.top/en/cs336-assign1/"><meta property="og:site_name" content="LoveIt"><meta property="og:title" content="[Stanford CS336] Assignment 1: Building a Transformer Language Model"><meta property="og:description" content="Why Should Systems Enthusiasts Learn Large Language Models? In today’s AI technology wave, mastering large model knowledge has become an essential skill for systems developers. By participating in Stanford CS336 Large Model Systems Course, I began my journey of building large models from scratch. This course is likely to become a landmark course in the systems field over the next 3 years (similar to the position of CMU 15-445 database course in recent years)."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-14T17:43:58+08:00"><meta property="article:modified_time" content="2026-01-30T02:27:52+08:00"><meta property="article:tag" content="CS336"><meta property="article:tag" content="LLM"><meta property="og:image" content="https://koschei.top/logo.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://koschei.top/logo.png"><meta name=twitter:title content="[Stanford CS336] Assignment 1: Building a Transformer Language Model"><meta name=twitter:description content="Why Should Systems Enthusiasts Learn Large Language Models? In today’s AI technology wave, mastering large model knowledge has become an essential skill for systems developers. By participating in Stanford CS336 Large Model Systems Course, I began my journey of building large models from scratch. This course is likely to become a landmark course in the systems field over the next 3 years (similar to the position of CMU 15-445 database course in recent years)."><meta name=twitter:site content="@xxxx"><meta name=application-name content="LoveIt"><meta name=apple-mobile-web-app-title content="LoveIt"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://koschei.top/en/cs336-assign1/><link rel=prev href=https://koschei.top/en/bpe-optimization/><link rel=next href=https://koschei.top/en/cs336-assign2/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"[Stanford CS336] Assignment 1: Building a Transformer Language Model","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/koschei.top\/en\/cs336-assign1\/"},"image":[{"@type":"ImageObject","url":"https:\/\/koschei.top\/images\/Apple-Devices-Preview.png","width":3200,"height":2048}],"genre":"posts","keywords":"CS336, LLM","wordcount":1468,"url":"https:\/\/koschei.top\/en\/cs336-assign1\/","datePublished":"2025-06-14T17:43:58+08:00","dateModified":"2026-01-30T02:27:52+08:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"xxxx","logo":{"@type":"ImageObject","url":"https:\/\/koschei.top\/images\/avatar.png","width":960,"height":960}},"author":{"@type":"Person","name":"Koschei"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/en/ title=LoveIt><span class=header-title-pre><i class='far fa-kiss-wink-heart fa-fw' aria-hidden=true></i></span>LoveIt</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/en/posts/>Posts </a><a class=menu-item href=/en/tags/>Tags </a><a class=menu-item href=/en/categories/>Categories </a><a class=menu-item href=/en/categories/documentation/>Docs </a><a class=menu-item href=/en/about/>About </a><a class=menu-item href=https://github.com/dillonzq/LoveIt title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i> </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="menu-item language" title="Select Language"><i class="fa fa-globe fa-fw" aria-hidden=true></i>
<select class=language-select id=language-select-desktop onchange="location=this.value"><option value=/en/cs336-assign1/ selected>English</option><option value=/cs336-assign1/>简体中文</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/en/ title=LoveIt><span class=header-title-pre><i class='far fa-kiss-wink-heart fa-fw' aria-hidden=true></i></span>LoveIt</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/en/posts/ title>Posts</a><a class=menu-item href=/en/tags/ title>Tags</a><a class=menu-item href=/en/categories/ title>Categories</a><a class=menu-item href=/en/categories/documentation/ title>Docs</a><a class=menu-item href=/en/about/ title>About</a><a class=menu-item href=https://github.com/dillonzq/LoveIt title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class=menu-item title="Select Language"><i class="fa fa-globe fa-fw" aria-hidden=true></i>
<select class=language-select onchange="location=this.value"><option value=/en/cs336-assign1/ selected>English</option><option value=/cs336-assign1/>简体中文</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">[Stanford CS336] Assignment 1: Building a Transformer Language Model</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/kosthi title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Koschei</a></span>&nbsp;<span class=post-category>included in <a href=/en/categories/llm/><i class="far fa-folder fa-fw" aria-hidden=true></i>LLM</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2025-06-14>2025-06-14</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;1468 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;7 minutes&nbsp;<span id=busuanzi_container_page_pv>
<i class="far fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_page_pv></span>&nbsp;views
</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#why-should-systems-enthusiasts-learn-large-language-models>Why Should Systems Enthusiasts Learn Large Language Models?</a><ul><li><a href=#assignment-overview>Assignment Overview</a></li></ul></li><li><a href=#1-byte-pair-encoding-bpe-tokenizer>1. Byte Pair Encoding (BPE) Tokenizer</a><ul><li><a href=#11-unicode-standard>1.1 Unicode Standard</a></li><li><a href=#12-unicode-encoding>1.2 Unicode Encoding</a></li><li><a href=#13-bpe-tokenizer-training-experiments>1.3 BPE Tokenizer Training Experiments</a></li><li><a href=#14-tokenizer-experiments>1.4 Tokenizer Experiments</a></li></ul></li><li><a href=#2-transformer-resource-accounting>2. Transformer Resource Accounting</a><ul><li><a href=#21-flops-accounting-basics>2.1 FLOPs Accounting Basics</a></li><li><a href=#22-gpt-2-xl-resource-accounting>2.2 GPT-2 XL Resource Accounting</a></li></ul></li><li><a href=#3-training-transformer-language-models>3. Training Transformer Language Models</a><ul><li><a href=#31-cross-entropy-loss>3.1 Cross-Entropy Loss</a></li><li><a href=#32-learning-rate-tuning>3.2 Learning Rate Tuning</a></li><li><a href=#33-implementing-adamw>3.3 Implementing AdamW</a></li></ul></li></ul></nav></div></div><div class=content id=content><h2 id=why-should-systems-enthusiasts-learn-large-language-models>Why Should Systems Enthusiasts Learn Large Language Models?</h2><p>In today&rsquo;s AI technology wave, mastering large model knowledge has become an essential skill for systems developers. By participating in <strong>Stanford CS336 Large Model Systems Course</strong>, I began my journey of building large models from scratch. This course is likely to become a landmark course in the systems field over the next 3 years (similar to the position of CMU 15-445 database course in recent years).</p><h3 id=assignment-overview>Assignment Overview</h3><p>This assignment implements a small language model through the following three modules:</p><ol><li><strong>Tokenizer Design and Implementation</strong> — Byte Pair Encoding (BPE) tokenizer</li><li><strong>Model Architecture Coding</strong> — Transformer with Self-Attention mechanism</li><li><strong>Optimizer Development</strong> — AdamW optimizer</li></ol><blockquote><p>Assignment link: <a href=https://github.com/Kosthi/assignment1-basics target=_blank rel="noopener noreffer">Assignment1-Basics GitHub Repository</a></p></blockquote><p>Next, I will share some details and insights from completing the assignment.</p><hr><h2 id=1-byte-pair-encoding-bpe-tokenizer>1. Byte Pair Encoding (BPE) Tokenizer</h2><h3 id=11-unicode-standard>1.1 Unicode Standard</h3><p><strong>Problem (unicode1): Understanding Unicode (1 point)</strong></p><p>(a) What Unicode character does chr(0) return?</p><p>The NULL character, i.e., ASCII null character.</p><p>(b) How does the string representation (&rsquo;<strong>repr</strong>()&rsquo;&rsquo;) differ from its printed representation?</p><p>The repr() function displays the escape sequence, while printing displays nothing (empty character).</p><p>(c) What happens when this character appears in text?</p><p>Although the null character is invisible when printed, it still exists as part of the Python string. This shows that Python strings can contain invisible characters, and these null characters still affect string storage and processing.</p><h3 id=12-unicode-encoding>1.2 Unicode Encoding</h3><p><strong>Problem (unicode2): Unicode Encoding (1 point)</strong></p><p>(a) Why do we prefer training tokenizers on UTF-8 encoded bytes rather than UTF-16 or UTF-32?</p><p>When training tokenizers, we process byte sequences. UTF-8 represents common characters more compactly, reducing sequence length, which is more efficient for model training. Moreover, UTF-8 is backward compatible with ASCII, making it especially efficient for processing English text.</p><p>(b) Why is this decode function incorrect?</p><p>This function is incorrect because it decodes byte by byte, which fails for multi-byte UTF-8 characters.</p><p>(c) Give a two-byte sequence that cannot be decoded.</p><p>0x80 0x81 is invalid because in UTF-8, any byte starting with binary 10 must be a continuation byte, but here it appears as the first byte.</p><h3 id=13-bpe-tokenizer-training-experiments>1.3 BPE Tokenizer Training Experiments</h3><p><strong>BPE Optimization Strategies</strong></p><p>To pursue high performance, I used Pybind11 to bind C++ code: pre-tokenization is handled by Python, while the BPE merge process is delegated to C++. Main optimizations include:</p><ol><li><strong>Parallelization</strong> — Use OpenMP for parallel statistics, avoiding lock contention</li><li><strong>Lazy Deletion Queue</strong> — Complexity reduced from O(NlogN) to O(KlogN)</li><li><strong>Incremental Updates</strong> — Only update affected adjacent pairs</li><li><strong>Efficient Data Structures</strong> — Integer IDs instead of strings, custom hash functions</li></ol><p><strong>Performance Comparison</strong> (TinyStoriesV2-GPT4-train dataset):</p><table><thead><tr><th>Version</th><th>BPE Merge Training</th><th>Speedup</th></tr></thead><tbody><tr><td>Python</td><td>10min++</td><td>Baseline</td></tr><tr><td>C++ Unoptimized</td><td>366s</td><td>~2x</td></tr><tr><td>C++ Optimized</td><td>1.08s</td><td><strong>300x</strong></td></tr></tbody></table><blockquote><p>For detailed optimization principles and implementation, see: <a href=/posts/bpe-optimization/ rel>High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second</a></p></blockquote><p><strong>Problem (train_bpe_tinystories): Train BPE on TinyStories (2 points)</strong></p><p>(a) How long did training take, and how much memory did it use? What is the longest token in the vocabulary? Is it reasonable?</p><ul><li>Training time: 28.03s</li><li>Memory: 10GB</li><li>Longest token: " accomplishment", ID: 7159, length: 15 characters (including leading space)</li><li>Reasonable: Yes</li></ul><p>(b) Analyze code performance. Which part of tokenizer training takes the longest?</p><ul><li><strong>N</strong>: Total distinct words</li><li><strong>L</strong>: Average word length</li><li><strong>V</strong>: Target vocabulary size</li><li><strong>M</strong>: Number of merges = V - 256 - |special_tokens|</li><li><strong>K</strong>: Occurrence count of specific pair</li><li><strong>P</strong>: Temporary pair frequency table</li></ul><p>Before optimization, the BPE merge process takes the longest (6 minutes), with time complexity O(M × N × L) and space complexity O(N × L + P).</p><p>After optimization, it takes only about 1s, with time complexity O(N × L + M).</p><p><strong>Problem (train_bpe_expts_owt): Train BPE on OpenWebText (2 points)</strong></p><p>(a) Train a byte-level BPE tokenizer on OpenWebText with max vocabulary size 32,000. What is the longest token? Is it reasonable?</p><p>Longest tokens:</p><ul><li>ID: 25835 | Byte length: 64 | Content: 64 hyphens</li><li>ID: 25821 | Byte length: 64 | Content: Repeated UTF-8 sequences</li></ul><p>(b) Compare tokenizers trained on TinyStories vs OpenWebText.</p><p>Some tokens contain newline characters, which when written to file without escaping, split a single merge rule across multiple lines.</p><h3 id=14-tokenizer-experiments>1.4 Tokenizer Experiments</h3><p><strong>Problem (tokenizer_experiments): Tokenizer Experiments (4 points)</strong></p><p>(a) Sample 10 documents each from TinyStories and OpenWebText. What is the compression ratio (bytes/token) for each tokenizer?</p><ul><li>TinyStories-10K tokenizer on TinyStories: <strong>4.14 bytes/token</strong></li><li>OpenWebText-32K tokenizer on OpenWebText: <strong>4.70 bytes/token</strong></li></ul><p>(b) What happens when encoding OpenWebText with the TinyStories tokenizer?</p><p>Using the TinyStories-10K tokenizer on OpenWebText documents, compression ratio drops to <strong>3.26 bytes/token</strong>, indicating the smaller vocabulary (10K) produces more tokens for complex text.</p><p>(c) Estimate tokenizer throughput.</p><ul><li>TinyStories-10K: ~626,519.6 bytes/sec, encoding 825GB Pile takes ~16.4 days</li><li>OpenWebText-32K: ~763,734.4 bytes/sec, takes ~13.4 days</li></ul><p>(d) Why is uint16 an appropriate choice for token IDs?</p><p>Both vocabularies (10K and 32K) are less than 65,536 (2^16), so uint16 suffices, saving 50% storage vs uint32.</p><hr><h2 id=2-transformer-resource-accounting>2. Transformer Resource Accounting</h2><h3 id=21-flops-accounting-basics>2.1 FLOPs Accounting Basics</h3><p>Understanding the computational and memory footprint of Transformer components is useful. We will perform basic &ldquo;FLOP accounting.&rdquo;</p><p>Most floating point operations in Transformers come from matrix multiplications, so the core idea is simple:</p><ol><li>List all matrix multiplication operations in Transformer forward pass</li><li>Convert each to required FLOPs</li></ol><blockquote><p><strong>Matrix Multiplication FLOPs Rule</strong>: For matrices A ∈ ℝ^(m×n) and B ∈ ℝ^(n×p), the multiplication AB requires 2mnp FLOPs.</p></blockquote><h3 id=22-gpt-2-xl-resource-accounting>2.2 GPT-2 XL Resource Accounting</h3><p><strong>Problem (transformer_accounting): Transformer LM Resource Accounting (5 points)</strong></p><p><strong>(a) GPT-2 XL Trainable Parameters</strong></p><p>GPT-2 XL configuration:</p><ul><li>vocab_size: 50,257</li><li>context_length: 1,024</li><li>num_layers: 48</li><li>d_model: 1,600</li><li>num_heads: 25</li><li>d_ff: 6,400</li></ul><p><strong>Token Embedding Layer</strong>: 50,257 × 1,600 = 80,411,200 (~80M)</p><p><strong>Single Transformer Block</strong>:</p><ul><li>MHA (Q/K/V + Output projections): 4 × 1,600 × 1,600 = 10,240,000 (~10M)</li><li>FFN (SwiGLU with W1, W2, W3): 3 × 6,400 × 1,600 = 30,720,000 (~31M)</li><li>2 RMSNorm layers: 2 × 1,600 = 3,200</li></ul><p>Total per layer: 40,963,200 (~41M)
48 layers total: 1,966,233,600 (~1.97B)</p><p><strong>Final RMSNorm</strong>: 1,600 parameters</p><p><strong>Output Projection (LM Head)</strong>: 50,257 × 1,600 = 80,411,200 (~80M)</p><p><strong>Total</strong>: ~2.13 billion parameters, requiring ~8 GB memory in float32.</p><p><strong>(b) List all matrix multiplications for forward pass FLOPs</strong></p><table><thead><tr><th>Operation</th><th>Dimensions</th></tr></thead><tbody><tr><td>MHA Q/K/V Projections</td><td>(seq, d_model) × (d_model, d_model) × 3 per layer</td></tr><tr><td>Attention Score Computation</td><td>(seq, d_k) × (d_k, seq) per head per layer</td></tr><tr><td>Attention Weighted Sum</td><td>(seq, seq) × (seq, d_v) per head per layer</td></tr><tr><td>MHA Output Projection</td><td>(seq, d_model) × (d_model, d_model) per layer</td></tr><tr><td>FFN Layers (W1, W2, W3)</td><td>(seq, d_model) × (d_model, d_ff) × 3 per layer</td></tr><tr><td>Output Layer (LM Head)</td><td>(seq, d_model) × (d_model, vocab_size)</td></tr></tbody></table><p>Total for seq_len = 1024: <strong>~4.20 TFLOPs</strong></p><p><strong>(c) Which parts consume most FLOPs?</strong></p><ul><li>Attention Q/K/V projections: 17.96%</li><li>Attention score/weighted sum: 0.30%</li><li>Attention output projection: 5.99%</li><li>FFN (all three layers): 71.83%</li><li>Output layer: 3.92%</li></ul><p><strong>FFN is the dominant component</strong> (~72% of FLOPs).</p><p><strong>(d) How do FLOPs distributions change with model scale?</strong></p><table><thead><tr><th>Component</th><th>GPT-2 Small</th><th>GPT-2 Medium</th><th>GPT-2 Large</th><th>GPT-2 XL</th></tr></thead><tbody><tr><td>MHA Q/K/V Projections</td><td>13.84%</td><td>16.51%</td><td>17.47%</td><td>17.96%</td></tr><tr><td>Attention Score/Sum</td><td>1.02%</td><td>0.68%</td><td>0.46%</td><td>0.30%</td></tr><tr><td>FFN (all layers)</td><td>55.36%</td><td>66.04%</td><td>69.89%</td><td>71.83%</td></tr><tr><td>Output Layer</td><td>25.16%</td><td>11.25%</td><td>6.35%</td><td>3.92%</td></tr></tbody></table><p><strong>Key trends</strong>:</p><ul><li>FFN proportion increases significantly with scale</li><li>Attention score computation proportion decreases sharply</li><li>Output layer proportion drops dramatically</li></ul><p><strong>(e) What happens if context_length increases to 16,384?</strong></p><ul><li>Total FLOPs increase from ~3.5T to ~70.4T (~20x increase, not linear due to O(S²) attention)</li><li>Attention module proportion increases from ~24% to ~28%</li><li>FFN proportion decreases from ~72% to ~69%</li></ul><hr><h2 id=3-training-transformer-language-models>3. Training Transformer Language Models</h2><h3 id=31-cross-entropy-loss>3.1 Cross-Entropy Loss</h3><p>The Transformer LM defines distribution p_θ(x_{i+1} | x_{1:i}) for sequence positions.</p><p>Cross-entropy (negative log-likelihood) loss:</p><p>85302
\ell( heta; D) = rac{1}{|D|} \sum_{x \in D} \sum_{i=1}^{m} -\log p_{ heta}(x_{i+1} | x_{1:i})
85302</p><p><strong>Problem (cross_entropy): Implement Cross-Entropy</strong></p><p>Implemented using numerically stable log_softmax.</p><h3 id=32-learning-rate-tuning>3.2 Learning Rate Tuning</h3><p><strong>Problem (learning_rate_tuning): Tune Learning Rate</strong></p><p>Testing SGD with different learning rates for 10 iterations:</p><ul><li>LR = 10: Loss slowly decays</li><li>LR = 100: Loss rapidly decays, converging near zero</li><li>LR = 1000: Loss explodes, clearly diverging</li></ul><h3 id=33-implementing-adamw>3.3 Implementing AdamW</h3><p><strong>Problem (adamw): Implement AdamW</strong></p><p>AdamW implementation with:</p><ul><li>First moment estimate (m)</li><li>Second moment estimate (v)</li><li>Bias correction</li><li>Decoupled weight decay</li></ul><p><strong>Problem (AdamW Accounting): AdamW Training Resource Accounting</strong></p><p><strong>(a) Peak Memory for AdamW</strong></p><ul><li>Parameters: 4(2Vd + L(16d² + 2d) + d) bytes</li><li>Gradients: Same as parameters</li><li>Optimizer state (m and v): 2× parameter memory</li><li>Activations: 4[L(16BTd + 2BhT²) + BTd + 2BTV] bytes</li></ul><p><strong>(b) For GPT-2 XL, what is max batch size in 80GB?</strong></p><p>Total memory ≈ 14.45B + 31.70 GB</p><p>Maximum batch size: <strong>3</strong></p><p><strong>(c) FLOPs for one AdamW step?</strong></p><p>~10 FLOPs per parameter for:</p><ol><li>First moment update: 2 FLOPs</li><li>Second moment update: 2 FLOPs</li><li>Bias correction: 2 FLOPs</li><li>Parameter update: 4 FLOPs</li></ol><p>Total: 10 × 2.13B = ~21.3 GFLOPs per step</p><p><strong>(d) Training time for GPT-2 XL on single A100?</strong></p><ul><li>FLOPs per step: 6 × B × T × P ≈ 1.34 × 10^16</li><li>Total FLOPs for 400K steps: ~5.35 × 10^21</li><li>A100 at 50% MFU: 9.75 × 10^12 FLOP/s</li><li>Training time: <strong>~17.4 years</strong> on single A100</li></ul></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2026-01-30&nbsp;<a class=git-hash href=https://github.com/dillonzq/LoveIt/commit/db19e4e838a7ab697d945feee127884ad8b14cb8 target=_blank title="commit by Koschei(nitianzero@gmail.com) db19e4e838a7ab697d945feee127884ad8b14cb8: feat: add English version blog">
<i class="fas fa-hashtag fa-fw" aria-hidden=true></i>db19e4e</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-markdown href=/en/cs336-assign1/index.md target=_blank>Read Markdown</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on X" data-sharer=x data-url=https://koschei.top/en/cs336-assign1/ data-title="[Stanford CS336] Assignment 1: Building a Transformer Language Model" data-via=xxxx data-hashtags=CS336,LLM><i class="fab fa-x-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://koschei.top/en/cs336-assign1/ data-hashtag=CS336><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://koschei.top/en/cs336-assign1/ data-title="[Stanford CS336] Assignment 1: Building a Transformer Language Model"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://koschei.top/en/cs336-assign1/ data-title="[Stanford CS336] Assignment 1: Building a Transformer Language Model"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@14.9.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://koschei.top/en/cs336-assign1/ data-title="[Stanford CS336] Assignment 1: Building a Transformer Language Model"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/en/tags/cs336/>CS336</a>,&nbsp;<a href=/en/tags/llm/>LLM</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/en/>Home</a></span></section></div><div class=post-nav><a href=/en/bpe-optimization/ class=prev rel=prev title="High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second</a>
<a href=/en/cs336-assign2/ class=next rel=next title="[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning">[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=valine class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://valine.js.org/>Valine</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div id=run-time data-start-time=2025-03-04T20:39:00+08:00 data-i18n-runtime="Running for <i class='far fa-clock fa-fw'></i> %%DAYS%% days %%HOURS%% hours %%MINUTES%% minutes %%SECONDS%% seconds"></div><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.155.0">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.3.0"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2025 - 2026</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://github.com/kosthi target=_blank>Koschei</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span><span class=icp-splitter>&nbsp;|&nbsp;</span><br class=icp-br><span class=icp><a href="https://beian.mps.gov.cn/#/query/webSearch?code=33032602100584" rel=noreferrer target=_blank><img src=备案图标.png alt=备案图标 style=vertical-align:middle;width:16px>浙公网安备33032602100584号</a></span><span class=icp-splitter>&nbsp;|&nbsp;</span><br class=icp-br><span class=icp><a href=https://beian.miit.gov.cn/ rel=noreferrer target=_blank>浙ICP备2024075396号-2</a></span></div><div class=busuanzi-footer><span id=busuanzi_container_site_pv><i class="fa fa-eye"></i> <span id=busuanzi_value_site_pv></span>
</span>&nbsp;|&nbsp;
<span id=busuanzi_container_site_uv><i class="fa fa-user"></i> <span id=busuanzi_value_site_uv></span></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i></a></div><div id=fixed-buttons-hidden><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=/lib/valine/valine.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css><script src=https://cdn.jsdelivr.net/npm/valine@1.5.3/dist/Valine.min.js></script><script src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@5.20.2/dist/lite/builds/browser.umd.min.js></script><script src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js></script><script src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js></script><script src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js></script><script src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.2/sharer.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/copy-tex.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/mhchem.min.js></script><script>window.config={comment:{valine:{appId:"QGzwQXOqs5JOhN4RGPOkR2mR-MdYXbMMI",appKey:"WBmoGyJtbqUswvfLh6L8iEBr",avatar:"mp",el:"#valine",emojiCDN:"https://cdn.jsdelivr.net/npm/emoji-datasource-google@15.1.2/img/google/64/",emojiMaps:{100:"1f4af.png",alien:"1f47d.png",anger:"1f4a2.png",angry:"1f620.png",anguished:"1f627.png",astonished:"1f632.png",black_heart:"1f5a4.png",blue_heart:"1f499.png",blush:"1f60a.png",bomb:"1f4a3.png",boom:"1f4a5.png",broken_heart:"1f494.png",brown_heart:"1f90e.png",clown_face:"1f921.png",cold_face:"1f976.png",cold_sweat:"1f630.png",confounded:"1f616.png",confused:"1f615.png",cry:"1f622.png",crying_cat_face:"1f63f.png",cupid:"1f498.png",dash:"1f4a8.png",disappointed:"1f61e.png",disappointed_relieved:"1f625.png",dizzy:"1f4ab.png",dizzy_face:"1f635.png",drooling_face:"1f924.png",exploding_head:"1f92f.png",expressionless:"1f611.png",face_vomiting:"1f92e.png",face_with_cowboy_hat:"1f920.png",face_with_hand_over_mouth:"1f92d.png",face_with_head_bandage:"1f915.png",face_with_monocle:"1f9d0.png",face_with_raised_eyebrow:"1f928.png",face_with_rolling_eyes:"1f644.png",face_with_symbols_on_mouth:"1f92c.png",face_with_thermometer:"1f912.png",fearful:"1f628.png",flushed:"1f633.png",frowning:"1f626.png",ghost:"1f47b.png",gift_heart:"1f49d.png",green_heart:"1f49a.png",grimacing:"1f62c.png",grin:"1f601.png",grinning:"1f600.png",hankey:"1f4a9.png",hear_no_evil:"1f649.png",heart:"2764-fe0f.png",heart_decoration:"1f49f.png",heart_eyes:"1f60d.png",heart_eyes_cat:"1f63b.png",heartbeat:"1f493.png",heartpulse:"1f497.png",heavy_heart_exclamation_mark_ornament:"2763-fe0f.png",hole:"1f573-fe0f.png",hot_face:"1f975.png",hugging_face:"1f917.png",hushed:"1f62f.png",imp:"1f47f.png",innocent:"1f607.png",japanese_goblin:"1f47a.png",japanese_ogre:"1f479.png",joy:"1f602.png",joy_cat:"1f639.png",kiss:"1f48b.png",kissing:"1f617.png",kissing_cat:"1f63d.png",kissing_closed_eyes:"1f61a.png",kissing_heart:"1f618.png",kissing_smiling_eyes:"1f619.png",laughing:"1f606.png",left_speech_bubble:"1f5e8-fe0f.png",love_letter:"1f48c.png",lying_face:"1f925.png",mask:"1f637.png",money_mouth_face:"1f911.png",nauseated_face:"1f922.png",nerd_face:"1f913.png",neutral_face:"1f610.png",no_mouth:"1f636.png",open_mouth:"1f62e.png",orange_heart:"1f9e1.png",partying_face:"1f973.png",pensive:"1f614.png",persevere:"1f623.png",pleading_face:"1f97a.png",pouting_cat:"1f63e.png",purple_heart:"1f49c.png",rage:"1f621.png",relaxed:"263a-fe0f.png",relieved:"1f60c.png",revolving_hearts:"1f49e.png",right_anger_bubble:"1f5ef-fe0f.png",robot_face:"1f916.png",rolling_on_the_floor_laughing:"1f923.png",scream:"1f631.png",scream_cat:"1f640.png",see_no_evil:"1f648.png",shushing_face:"1f92b.png",skull:"1f480.png",skull_and_crossbones:"2620-fe0f.png",sleeping:"1f634.png",sleepy:"1f62a.png",slightly_frowning_face:"1f641.png",slightly_smiling_face:"1f642.png",smile:"1f604.png",smile_cat:"1f638.png",smiley:"1f603.png",smiley_cat:"1f63a.png",smiling_face_with_3_hearts:"1f970.png",smiling_imp:"1f608.png",smirk:"1f60f.png",smirk_cat:"1f63c.png",sneezing_face:"1f927.png",sob:"1f62d.png",space_invader:"1f47e.png",sparkling_heart:"1f496.png",speak_no_evil:"1f64a.png",speech_balloon:"1f4ac.png","star-struck":"1f929.png",stuck_out_tongue:"1f61b.png",stuck_out_tongue_closed_eyes:"1f61d.png",stuck_out_tongue_winking_eye:"1f61c.png",sunglasses:"1f60e.png",sweat:"1f613.png",sweat_drops:"1f4a6.png",sweat_smile:"1f605.png",thinking_face:"1f914.png",thought_balloon:"1f4ad.png",tired_face:"1f62b.png",triumph:"1f624.png",two_hearts:"1f495.png",unamused:"1f612.png",upside_down_face:"1f643.png",weary:"1f629.png",white_frowning_face:"2639-fe0f.png",white_heart:"1f90d.png",wink:"1f609.png",woozy_face:"1f974.png",worried:"1f61f.png",yawning_face:"1f971.png",yellow_heart:"1f49b.png",yum:"1f60b.png",zany_face:"1f92a.png",zipper_mouth_face:"1f910.png",zzz:"1f4a4.png"},enableQQ:!1,highlight:!0,lang:"en",pageSize:10,placeholder:"Your comment ...",recordIP:!0,serverURLs:"https://valine.hugoloveit.com",visitor:!0}},lightgallery:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{algoliaAppID:"4D1IDY8JU6",algoliaIndex:"index",algoliaSearchKey:"05332ac5ed76655a511f0da583a9afac",highlightTag:"em",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"algolia"}}</script><script src=/js/theme.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/jquery@2.1.3/dist/jquery.min.js></script><script type=text/javascript src=/js/custom.js></script></body></html>