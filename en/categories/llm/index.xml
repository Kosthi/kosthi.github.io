<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLM on LoveIt</title><link>https://koschei.top/en/categories/llm/</link><description>Recent content in LLM on LoveIt</description><generator>Hugo</generator><language>en</language><managingEditor>nitianzero@gmail.com (Koschei)</managingEditor><webMaster>nitianzero@gmail.com (Koschei)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sat, 31 Jan 2026 01:43:03 +0800</lastBuildDate><atom:link href="https://koschei.top/en/categories/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>[Stanford CS336] Assignment 5: Alignment and Reasoning Reinforcement Learning</title><link>https://koschei.top/en/cs336-assign5/</link><pubDate>Tue, 13 Jan 2026 18:12:00 +0800</pubDate><author>nitianzero@gmail.com (Koschei)</author><guid>https://koschei.top/en/cs336-assign5/</guid><description>&lt;h2 id="1-assignment-overview"&gt;1 Assignment Overview&lt;/h2&gt;
&lt;p&gt;In this assignment, you will gain hands-on experience in training language models to reason when solving math problems.&lt;/p&gt;
&lt;h3 id="what-to-implement"&gt;What to Implement&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Implement a zero-shot prompting baseline for the MATH competition dataset proposed by Hendrycks et al. [2021].&lt;/li&gt;
&lt;li&gt;Supervised Fine-Tuning (SFT) using reasoning traces from a stronger reasoning model (DeepSeek R1, DeepSeekAI et al., 2025).&lt;/li&gt;
&lt;li&gt;Use Expert Iteration to improve reasoning performance through verification rewards.&lt;/li&gt;
&lt;li&gt;Use Group Relative Policy Optimization (GRPO) to improve reasoning performance through verification rewards.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For interested students, we will release an optional part of the assignment in the coming days: aligning language models with human preferences.&lt;/p&gt;</description></item><item><title>[Stanford CS336] Assignment 1: Building a Transformer Language Model</title><link>https://koschei.top/en/cs336-assign1/</link><pubDate>Sat, 14 Jun 2025 17:43:58 +0800</pubDate><author>nitianzero@gmail.com (Koschei)</author><guid>https://koschei.top/en/cs336-assign1/</guid><description>&lt;h2 id="why-should-systems-enthusiasts-learn-large-language-models"&gt;Why Should Systems Enthusiasts Learn Large Language Models?&lt;/h2&gt;
&lt;p&gt;In today&amp;rsquo;s AI technology wave, mastering large model knowledge has become an essential skill for systems developers. By participating in &lt;strong&gt;Stanford CS336 Large Model Systems Course&lt;/strong&gt;, I began my journey of building large models from scratch. This course is likely to become a landmark course in the systems field over the next 3 years (similar to the position of CMU 15-445 database course in recent years).&lt;/p&gt;</description></item><item><title>High-Performance BPE Tokenizer Optimization: From 10 Minutes to 1 Second</title><link>https://koschei.top/en/bpe-optimization/</link><pubDate>Sat, 14 Jun 2025 17:43:58 +0800</pubDate><author>nitianzero@gmail.com (Koschei)</author><guid>https://koschei.top/en/bpe-optimization/</guid><description>&lt;blockquote&gt;
&lt;p&gt;This article is supplementary reading for &lt;a href="https://koschei.top/posts/cs336-assign1/" rel=""&gt;CS336 Assignment 1&lt;/a&gt;, providing a detailed introduction to the optimized implementation of the BPE tokenizer.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;The recommended cppyy in the documentation has issues in Mac and Linux environments. To pursue high performance, I used Pybind11 to bind C++ code: pre-tokenization is handled by Python, while the BPE merge process is delegated to C++. The actual biggest bottleneck is still pre-tokenization, which can be parallelized using the existing code &lt;code&gt;pretokenization_example.py&lt;/code&gt; for chunked parallel processing (8 cores 100s â†’ 16 cores 30s).&lt;/p&gt;</description></item></channel></rss>